{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0abefc9-aa59-425b-ac94-8150d9e392ba",
   "metadata": {},
   "source": [
    "# Experiment 5.1: Information Bottleneck Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc33fc5f-e1fc-4016-b60e-f449ae72eac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "jax.config.update(\"jax_default_matmul_precision\", \"highest\")\n",
    "\n",
    "from src.equations import ac_res as pde_res\n",
    "from jaxkan.utils.PIKAN import gradf\n",
    "\n",
    "from src.utils import _get_adam, _get_pde_collocs, _get_ic_collocs, model_eval, count_params, _get_colloc_indices, grad_norm\n",
    "from src.kan import KAN\n",
    "from src.rgakan import RGAKAN\n",
    "\n",
    "import numpy as np\n",
    "from jax import device_get\n",
    "\n",
    "import optax\n",
    "from flax import nnx\n",
    "\n",
    "import os\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "results_dir = \"results\"\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "plots_dir = \"plots\"\n",
    "os.makedirs(plots_dir, exist_ok=True)\n",
    "        \n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48f2a30-c810-48fa-ba8c-bef46882c9ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f39b38a3-4ded-4666-883b-184e6ff83513",
   "metadata": {},
   "source": [
    "## Basic Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edafc74-0177-4e4d-a83e-58f7c54e5b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Causal Weights with RBA contribution\n",
    "def get_causal_weights(model, collocs, l_E):\n",
    "    \n",
    "    residuals = pde_res(model, collocs)\n",
    "\n",
    "    # Apply RBA to residuals\n",
    "    w_resids = l_E * residuals\n",
    "    \n",
    "    # Reshape to (num_chunks, points)\n",
    "    w_resids = w_resids.reshape(num_chunks, -1)\n",
    "    \n",
    "    # Get average loss per chunk\n",
    "    loss = jnp.mean(w_resids**2, axis=1)\n",
    "    \n",
    "    # Get causal weights\n",
    "    weights = jnp.exp(-causal_tol * (M @ loss))\n",
    "    \n",
    "    points_per_chunk = collocs.shape[0] // num_chunks\n",
    "    extended_weights = jnp.repeat(weights, points_per_chunk)\n",
    "    \n",
    "    return jax.lax.stop_gradient(extended_weights)\n",
    "\n",
    "\n",
    "# PDE Loss (E)\n",
    "def single_pde_loss(model, weight, x):\n",
    "\n",
    "    # Eq. parameter\n",
    "    D = jnp.array(1e-4, dtype=jnp.float32)\n",
    "    c = jnp.array(5.0, dtype=jnp.float32)\n",
    "\n",
    "    def u(t):\n",
    "        y = model(t)\n",
    "        return y\n",
    "\n",
    "    # Physics Loss Terms\n",
    "    u_t = gradf(u, 0, 1)\n",
    "    u_xx = gradf(u, 1, 2)\n",
    "\n",
    "    # Get all residuals\n",
    "    pde_res = u_t(x) - D*u_xx(x) - c*(u(x)-(u(x)**3))\n",
    "\n",
    "    pde_res = pde_res.flatten()\n",
    "\n",
    "    # Get average loss per chunk\n",
    "    loss = weight * pde_res[0]**2\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "# IC Loss (I)\n",
    "def single_ic_loss(model, weight, x, y):\n",
    "\n",
    "    # Residual\n",
    "    ic_res = model(x) - y\n",
    "    \n",
    "    r = ic_res.flatten()[0]\n",
    "    \n",
    "    w = jax.lax.stop_gradient(weight)\n",
    "    loss = w * (r**2)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "@nnx.jit\n",
    "def snr_train_step(model, optimizer, collocs, ic_collocs, ic_data, λ_E, λ_I, l_E, l_I):\n",
    "\n",
    "    # ----------------------- PDE --------------------------------------------- #\n",
    "\n",
    "    # RBA updates\n",
    "    res = pde_res(model, collocs)\n",
    "    abs_res = jnp.abs(res)\n",
    "    l_E_new = (RBA_gamma * l_E) + (RBA_eta * (abs_res/jnp.max(abs_res)))\n",
    "\n",
    "    # causal weights computed on RBA-weighted residuals\n",
    "    causal_w = get_causal_weights(model, collocs, l_E_new)\n",
    "    total_w_E = causal_w * l_E_new.flatten()\n",
    "\n",
    "    # Define a batched PDE loss\n",
    "    batched_E = nnx.vmap(nnx.value_and_grad(single_pde_loss), in_axes=(None, 0, 0))\n",
    "    # Get all (weighted) losses and all gradients\n",
    "    all_loss_E, all_grads_E = batched_E(model, total_w_E, collocs[:, None, :])\n",
    "    # Get average PDE loss\n",
    "    loss_E = jnp.mean(all_loss_E)\n",
    "    # Get weighted gradients\n",
    "    all_grads_E = jax.tree_util.tree_map(lambda g: λ_E * g, all_grads_E)\n",
    "    # Get average gradients for Loss Annealing\n",
    "    grads_E = jax.tree_util.tree_map(lambda g: jnp.mean(g, axis=0), all_grads_E)\n",
    "\n",
    "\n",
    "    # ----------------------- ICs --------------------------------------------- #\n",
    "\n",
    "    # RBA updates\n",
    "    ic_res_full = model(ic_collocs) - ic_data\n",
    "    abs_ic = jnp.abs(ic_res_full)\n",
    "    l_I_new = (RBA_gamma * l_I) + (RBA_eta * (abs_ic/jnp.max(abs_ic)))\n",
    "\n",
    "    # Define a batched ICs loss\n",
    "    batched_I = nnx.vmap(nnx.value_and_grad(single_ic_loss), in_axes=(None, 0, 0, 0))\n",
    "    # Get all losses and all gradients\n",
    "    all_loss_I, all_grads_I = batched_I(model, l_I_new.flatten(), ic_collocs[:, None, :], ic_data[:, None, :])\n",
    "    # Get average IC loss\n",
    "    loss_I = jnp.mean(all_loss_I)\n",
    "    # Get weighted gradients\n",
    "    all_grads_I = jax.tree_util.tree_map(lambda g: λ_I * g, all_grads_I)\n",
    "    # Get average gradients for Loss Annealing\n",
    "    grads_I = jax.tree_util.tree_map(lambda g: jnp.mean(g, axis=0), all_grads_I)\n",
    "\n",
    "    # --------------------- TOTAL -----------------------#\n",
    "\n",
    "    # Compute total loss\n",
    "    loss = λ_E*loss_E + λ_I*loss_I\n",
    "    # Compute total gradients\n",
    "    grads = jax.tree_util.tree_map(lambda g1, g2: g1 + g2, grads_E, grads_I)\n",
    "    # Update optimizer\n",
    "    optimizer.update(grads)\n",
    "\n",
    "    # Get mean and std for SNR\n",
    "    all_grads = jax.tree_util.tree_map(lambda g1, g2: jnp.concatenate([g1, g2], axis=0), all_grads_E, all_grads_I)\n",
    "\n",
    "    mean_grads = jax.tree_util.tree_map(lambda g: jnp.mean(g, axis=0), all_grads)\n",
    "    std_grads = jax.tree_util.tree_map(lambda g: jnp.std(g, axis=0),  all_grads)\n",
    "    \n",
    "    mu = optax.global_norm(mean_grads)\n",
    "    sd = optax.global_norm(std_grads)\n",
    "    snr = mu / (sd + 1e-8)\n",
    "    \n",
    "    return loss, grads_E, grads_I, snr, l_E_new, l_I_new\n",
    "\n",
    "\n",
    "@nnx.jit\n",
    "def get_RAD_indices(model, collocs_pool, old_indices, l_E, l_E_pool):\n",
    "\n",
    "    # Apply updates from old indices to pool\n",
    "    updated_pool = l_E_pool.at[old_indices].set(l_E)\n",
    "\n",
    "    # Get full residuals\n",
    "    resids = pde_res(model, collocs_pool)\n",
    "    \n",
    "    # Multiply by RBA weights\n",
    "    w_resids = updated_pool * resids\n",
    "    \n",
    "    # Get absolute\n",
    "    wa_resids = jnp.abs(w_resids)\n",
    "\n",
    "    # Raise to power rad_a\n",
    "    ea = jnp.power(wa_resids, rad_a)\n",
    "    \n",
    "    # Divide by mean and add rad_c\n",
    "    px = (ea/jnp.mean(ea)) + rad_c\n",
    "    \n",
    "    # Normalize\n",
    "    px_norm = (px / jnp.sum(px))[:,0]\n",
    "\n",
    "    sorted_indices = _get_colloc_indices(collocs_pool=collocs_pool, batch_size=batch_size, px=px_norm, seed=seed)\n",
    "\n",
    "    return sorted_indices, updated_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b87b92e-1a77-4a94-91e3-bde4e1ef86a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frob(model, x):\n",
    "\n",
    "    # normalize x to (1, d) for the model\n",
    "    x = x[None, :] if x.ndim == 1 else x\n",
    "\n",
    "    def u(t):\n",
    "        y = model(t).flatten()\n",
    "        return y[0]\n",
    "    \n",
    "    g = jax.grad(u)(x)\n",
    "    fro_sq = jnp.vdot(g, g)\n",
    "    \n",
    "    return fro_sq\n",
    "\n",
    "batched_frob = nnx.jit(jax.vmap(get_frob, in_axes=(None, 0)))\n",
    "\n",
    "def get_complexity(model, collocs, ic_collocs):\n",
    "    combined = jnp.concatenate([collocs, ic_collocs], axis=0)\n",
    "    frob = jnp.mean(batched_frob(model, combined))\n",
    "    return frob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b843d3e-798c-415c-beef-930f55e7ff32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e6d14ece-acba-44d0-a1ac-082c53f5bc7b",
   "metadata": {},
   "source": [
    "### Data & Grid-Search Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e62fcf8-43a0-4a55-aa2b-3e26b09842b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collocation points and ICs\n",
    "collocs_pool = _get_pde_collocs(ranges = [(0,1), (-1,1)], sample_size = 400)\n",
    "ic_collocs = _get_ic_collocs(x_range = (-1, 1), sample_size = 2**6)\n",
    "ic_data = ((ic_collocs[:,1]**2)*jnp.cos(jnp.pi*ic_collocs[:,1])).reshape(-1,1)\n",
    "\n",
    "# Reference solution\n",
    "ref = np.load('data/allen_cahn.npz')\n",
    "refsol = jnp.array(ref['usol'])\n",
    "\n",
    "N_t, N_x = ref['usol'].shape\n",
    "t, x = ref['t'].flatten(), ref['x'].flatten()\n",
    "T, X = jnp.meshgrid(t, x, indexing='ij')\n",
    "coords = jnp.hstack([T.flatten()[:, None], X.flatten()[:, None]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e17f39d-8f93-417a-9a2d-7639ca55dcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training epochs\n",
    "num_epochs = 100_000\n",
    "\n",
    "# Scheduler configurations\n",
    "learning_rate = 1e-3\n",
    "decay_steps = 2000\n",
    "decay_rate = 0.9\n",
    "warmup_steps = 1000\n",
    "\n",
    "# Define causal training parameters\n",
    "causal_tol = 1.0\n",
    "num_chunks = 32\n",
    "M = jnp.triu(jnp.ones((num_chunks, num_chunks)), k=1).T\n",
    "\n",
    "# Define Grad Norm parameters\n",
    "grad_mixing = 0.9\n",
    "f_grad_norm = 1000\n",
    "\n",
    "# Define resampling parameters\n",
    "batch_size = 2**12\n",
    "f_resample = 2000\n",
    "rad_a = 1.0\n",
    "rad_c = 1.0\n",
    "\n",
    "# Define RBA parameters\n",
    "RBA_gamma = 0.999\n",
    "RBA_eta = 0.01\n",
    "\n",
    "# Define model parameters\n",
    "n_in = collocs_pool.shape[1]\n",
    "n_out = 1\n",
    "D = 5\n",
    "period_axes = {1: jnp.pi}\n",
    "rff_std = None #2.0\n",
    "sine_D = 5 #None\n",
    "alpha = 0.0\n",
    "beta = 0.0\n",
    "init_scheme = {'type': 'glorot', 'gain': None, 'norm_pow': 0, 'distribution': 'uniform', 'sample_size': 10000}\n",
    "\n",
    "num_blocks = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ea0467-fec4-4aab-a16d-4f0e76d9e245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Architecture settings\n",
    "widths = [8, 16, 32]\n",
    "archs = [\"RGA\", \"cPIKAN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdba72aa-e355-4b40-8896-a0cdc606a602",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "50f01465-46ff-4e4e-8f06-2a535ed6cab0",
   "metadata": {},
   "source": [
    "### Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a1a9d0-53ef-4e3b-9450-ebeb58c792c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS = dict()\n",
    "\n",
    "# Grid search over width size\n",
    "for n_hidden in widths:\n",
    "    RESULTS[n_hidden] = dict()\n",
    "\n",
    "    if n_hidden in [16, 32]:\n",
    "        run = 7 # Worst performing run for RGAKANs with widths = 16, 32 from previous experiment\n",
    "    else:\n",
    "        run = 13 # Worst performing run for RGAKAN with width = 8 from previous experiment\n",
    "    \n",
    "    print(f\"Training models with depth = {int(2*num_blocks)} and width = {n_hidden}.\")\n",
    "\n",
    "    # Grid search over architecture types\n",
    "    for arch in archs:\n",
    "        RESULTS[n_hidden][arch] = dict()\n",
    "\n",
    "        RESULTS[n_hidden][arch]['L2'] = []\n",
    "        RESULTS[n_hidden][arch]['SNR'] = []\n",
    "        RESULTS[n_hidden][arch]['C'] = []\n",
    "        \n",
    "        # Initialize RBA weights - full pool\n",
    "        l_E_pool = jnp.ones((collocs_pool.shape[0], 1))\n",
    "        # Also get RBAs for ICs\n",
    "        l_I = jnp.ones((ic_collocs.shape[0], 1))\n",
    "    \n",
    "        # Get starting collocation points & RBA weights\n",
    "        sorted_indices = _get_colloc_indices(collocs_pool=collocs_pool, batch_size=batch_size, px=None, seed=seed)\n",
    "        \n",
    "        collocs = collocs_pool[sorted_indices]\n",
    "        l_E = l_E_pool[sorted_indices]\n",
    "        \n",
    "        # Get opt_type\n",
    "        opt_type = _get_adam(learning_rate=learning_rate, decay_steps=decay_steps, decay_rate=decay_rate, warmup_steps=warmup_steps)\n",
    "\n",
    "        # Define model\n",
    "        if arch == \"RGA\":\n",
    "            model = RGAKAN(n_in=n_in, n_out=n_out, n_hidden=n_hidden, num_blocks=num_blocks, D=D,\n",
    "                           init_scheme=init_scheme, alpha=alpha, beta=beta, ref=ref, period_axes=period_axes, rff_std=rff_std,\n",
    "                           sine_D=sine_D, seed=seed+run)\n",
    "        elif arch == \"cPIKAN\":\n",
    "            model = KAN(n_in=n_in, n_out=n_out, n_hidden=n_hidden, num_layers=int(2*num_blocks), D=D,\n",
    "                        init_scheme=init_scheme, period_axes=period_axes, rff_std=rff_std, seed=seed+run)\n",
    "\n",
    "        print(f\"Initialized {arch} model with {count_params(model)} parameters.\")\n",
    "        \n",
    "        # Define global loss weights\n",
    "        λ_E = jnp.array(1.0, dtype=float)\n",
    "        λ_I = jnp.array(1.0, dtype=float)\n",
    "\n",
    "        # Set optimizer\n",
    "        optimizer = nnx.Optimizer(model, opt_type)\n",
    "    \n",
    "        # Start training\n",
    "        for epoch in range(num_epochs):\n",
    "        \n",
    "            loss, grads_E, grads_I, snr, l_E, l_I = snr_train_step(model, optimizer, collocs, ic_collocs, ic_data, λ_E, λ_I, l_E, l_I)\n",
    "\n",
    "            # Perform grad norm\n",
    "            if (epoch != 0) and (epoch % f_grad_norm == 0):\n",
    "        \n",
    "                λ_Ε, λ_I = grad_norm(grads_E, grads_I, λ_E, λ_I, grad_mixing)\n",
    "        \n",
    "            # Perform RAD\n",
    "            if (epoch != 0) and (epoch % f_resample == 0):\n",
    "    \n",
    "                # Get new indices after resampling\n",
    "                sorted_indices, l_E_pool = get_RAD_indices(model, collocs_pool, sorted_indices, l_E, l_E_pool)\n",
    "                # Set new batch of collocs and l_E\n",
    "                collocs = collocs_pool[sorted_indices]\n",
    "                l_E = l_E_pool[sorted_indices]\n",
    "\n",
    "            # Calculate L2 Error at the end of EACH epoch\n",
    "            l2error = model_eval(model, coords, refsol)\n",
    "            # Get complexity\n",
    "            compl = get_complexity(model, collocs, ic_collocs)\n",
    "                \n",
    "            RESULTS[n_hidden][arch]['L2'].append(l2error.item())\n",
    "            RESULTS[n_hidden][arch]['SNR'].append(snr.item())\n",
    "            RESULTS[n_hidden][arch]['C'].append(compl.item())\n",
    "\n",
    "            if epoch % 5000 == 0:\n",
    "                    print(f\"Epoch: {epoch}. Current snr = {snr:.2e}, Current L2 Error = {l2error:.2e}, Current complexity = {compl:.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5418783-ad75-4fa6-821a-01aad11aac1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "snr_file = os.path.join(results_dir, \"snr.pkl\")\n",
    "\n",
    "with open(snr_file, \"wb\") as f:\n",
    "    pickle.dump(RESULTS, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae68325e-f212-4239-ad38-800505d21440",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3d03e086-2e98-4104-a2a3-ce0ee8bb9012",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094b4b04-d6b6-4438-b666-0922034c79a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "filepath = os.path.join(results_dir, \"snr.pkl\")\n",
    "\n",
    "with open(filepath, \"rb\") as f:\n",
    "    RESULTS = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9188782-400a-426c-9f44-9f853f9cf673",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import LogLocator, LogFormatterMathtext, NullLocator, NullFormatter\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_snr_grid(RESULTS, vlines=None, widths_order=(8, 16, 32), savepath=None, filename=None):\n",
    "\n",
    "    # ---- styling: reuse your paper's knobs if present; else set sane fallbacks ----\n",
    "    TITLE_FS   = globals().get(\"TITLE_FS\", 16)\n",
    "    LABEL_FS   = globals().get(\"LABEL_FS\", 16)\n",
    "    TICK_FS    = globals().get(\"TICK_FS\", 14)\n",
    "    LEGEND_FS  = globals().get(\"LEGEND_FS\", 16)\n",
    "    LINE_W     = globals().get(\"LINE_W\", 2.0)\n",
    "\n",
    "    # colors to stay consistent with your other plots\n",
    "    _cmap = sns.color_palette(\"Spectral\", as_cmap=True)\n",
    "    cmap_points = np.linspace(0, 1, 12)\n",
    "    color_indices = [1, -2]  # two distinct tones\n",
    "    _colors = [_cmap(cmap_points[i]) for i in color_indices]\n",
    "\n",
    "    # label mapping to final legend text\n",
    "    MODEL_LABEL = {\"cPIKAN\": \"cPIKAN\", \"RGA\": \"RGA KAN\"}\n",
    "\n",
    "    metrics = [\"L2\", \"SNR\", \"C\"]\n",
    "    ylabels = [r\"Relative $L^2$ Error\", \"SNR\", \"Complexity\"]\n",
    "\n",
    "    # figure layout: rows = metrics, cols = widths present in RESULTS\n",
    "    widths = [w for w in widths_order if w in RESULTS.keys()]\n",
    "    if not widths:\n",
    "        raise ValueError(\"No matching widths found in RESULTS.\")\n",
    "\n",
    "    nrows, ncols = len(metrics), len(widths)\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows, ncols, figsize=(4.2*ncols, 2.2*nrows),\n",
    "        sharex=True, sharey=False, constrained_layout=True\n",
    "    )\n",
    "    fig.set_constrained_layout_pads(w_pad=0.08, h_pad=0.08, hspace=0.06, wspace=0.06)\n",
    "\n",
    "    # normalize axes to 2D\n",
    "    if nrows == 1 and ncols == 1:\n",
    "        axes = np.array([[axes]])\n",
    "    elif nrows == 1:\n",
    "        axes = axes[np.newaxis, :]\n",
    "    elif ncols == 1:\n",
    "        axes = axes[:, np.newaxis]\n",
    "\n",
    "    # plot\n",
    "    legend_handles = None\n",
    "    for c, width in enumerate(widths):\n",
    "        col_data = RESULTS[width]\n",
    "\n",
    "        for r, (metric, ylabel) in enumerate(zip(metrics, ylabels)):\n",
    "            ax = axes[r, c]\n",
    "\n",
    "            # x = epochs\n",
    "            # use max length among present models to define x range\n",
    "            lengths = []\n",
    "            for arch in (\"cPIKAN\", \"RGA\"):\n",
    "                if arch in col_data and metric in col_data[arch]:\n",
    "                    lengths.append(len(col_data[arch][metric]))\n",
    "            if not lengths:\n",
    "                continue\n",
    "            xmax = max(lengths)\n",
    "            x = np.arange(1, xmax + 1)\n",
    "\n",
    "            # plot in consistent order with consistent colors\n",
    "            lines_here = []\n",
    "            for j, arch in enumerate((\"cPIKAN\", \"RGA\")):\n",
    "                if arch not in col_data or metric not in col_data[arch]:\n",
    "                    continue\n",
    "                y = np.asarray(col_data[arch][metric], dtype=float)\n",
    "                xx = np.arange(1, len(y) + 1)\n",
    "                h, = ax.plot(xx, y, linewidth=LINE_W, color=_colors[j % len(_colors)],\n",
    "                             label=MODEL_LABEL.get(arch, arch))\n",
    "                lines_here.append(h)\n",
    "\n",
    "            # vertical markers (optional)\n",
    "            if vlines and width in vlines:\n",
    "                for v in vlines[width]:\n",
    "                    ax.axvline(v, linestyle=\"--\", color=\"black\", alpha=0.7, linewidth=1.0)\n",
    "\n",
    "            # scales, ticks, grid\n",
    "            ax.set_xscale(\"log\")\n",
    "            # match your log y-axis formatting helper if available\n",
    "            if \" _set_log_ticks\" in globals() and callable(globals().get(\"_set_log_ticks\")):\n",
    "                globals()[\"_set_log_ticks\"](ax)\n",
    "            else:\n",
    "                ax.set_yscale(\"log\")\n",
    "                ax.tick_params(axis=\"both\", which=\"both\", labelsize=TICK_FS)\n",
    "\n",
    "            ax.grid(True, which=\"both\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "            # labels\n",
    "            if c == 0:\n",
    "                ax.set_ylabel(ylabel, fontsize=LABEL_FS)\n",
    "            if r == nrows - 1:\n",
    "                ax.set_xlabel(\"Training Iteration\", fontsize=LABEL_FS)\n",
    "            if r == 0:\n",
    "                ax.set_title(f\"Width = {width}\", fontsize=TITLE_FS)\n",
    "\n",
    "            if legend_handles is None and lines_here:\n",
    "                legend_handles = lines_here\n",
    "\n",
    "    # common legend (models) below all subplots\n",
    "    if legend_handles:\n",
    "        fig.legend(\n",
    "            legend_handles, [h.get_label() for h in legend_handles],\n",
    "            loc=\"lower center\", ncol=len(legend_handles), frameon=False, fontsize=LEGEND_FS,\n",
    "            bbox_to_anchor=(0.5, -0.10)\n",
    "        )\n",
    "\n",
    "    # save\n",
    "    if savepath is not None and filename is not None:\n",
    "        plt.savefig(os.path.join(savepath, f\"{filename}.pdf\"), format=\"pdf\", bbox_inches=\"tight\")\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b886dc2-c8cb-47ba-bfae-c99055e76ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vlines = {\n",
    "    8: (230, 9500),\n",
    "    16: (200, 7000),\n",
    "    32: (140, 4500)\n",
    "}\n",
    "\n",
    "plot_snr_grid(RESULTS, vlines, widths_order=(8, 16, 32), savepath=plots_dir, filename=\"snr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbefeec-3e77-4a27-8af0-0587c3134b0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
