{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5edbf20f-f09e-4919-a5eb-91aaffdc0b5a",
   "metadata": {},
   "source": [
    "# Experiment 14.1: Navier-Stokes Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02759ddc-89c8-4cf9-b8ff-3470b51bebc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "jax.config.update(\"jax_default_matmul_precision\", \"highest\")\n",
    "\n",
    "from src.equations import nst_res as pde_res\n",
    "from src.equations import nst_w_func\n",
    "\n",
    "from src.utils import _get_adam, count_params, _get_colloc_indices, grad_norm\n",
    "from src.utils import count_rga, count_pirate, count_pikan\n",
    "from src.rgakan import RGAKAN\n",
    "from src.kan import KAN\n",
    "from src.piratenet import PirateNet\n",
    "\n",
    "import numpy as np\n",
    "from jax import device_get\n",
    "\n",
    "import optax\n",
    "from flax import nnx\n",
    "\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import os\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "results_dir = \"results\"\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "result_file = os.path.join(results_dir, \"benchmarks_nst.pkl\")\n",
    "\n",
    "plots_dir = \"plots\"\n",
    "os.makedirs(plots_dir, exist_ok=True)\n",
    "\n",
    "RESULTS = dict()\n",
    "        \n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c22456",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e57ace9",
   "metadata": {},
   "source": [
    "## Basic Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10ccf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDE Loss - returns 2 residuals: vorticity transport and continuity\n",
    "def pde_loss(model, l_E, collocs):\n",
    "\n",
    "    residuals = pde_res(model, collocs, Re=Re)  # shape (batch_size, 2): [vorticity, continuity]\n",
    "\n",
    "    # Get new RBA weights (use mean of residuals for weighting)\n",
    "    abs_res = jnp.abs(residuals).mean(axis=1, keepdims=True)  # shape (batch_size, 1)\n",
    "    l_E_new = (RBA_gamma*l_E) + (RBA_eta*abs_res/jnp.max(abs_res))\n",
    "\n",
    "    # Multiply by RBA weights\n",
    "    w_resids = l_E_new * residuals  # shape (batch_size, 2)\n",
    "\n",
    "    # Reshape residuals for causal training (along time dimension)\n",
    "    # Shape: (num_chunks, points_per_chunk, 2)\n",
    "    resids_chunked = w_resids.reshape(num_chunks, -1, 2)\n",
    "\n",
    "    # Get average loss per chunk for each residual type\n",
    "    loss_vort = jnp.mean(resids_chunked[:, :, 0]**2, axis=1)  # shape (num_chunks,)\n",
    "    loss_cont = jnp.mean(resids_chunked[:, :, 1]**2, axis=1)  # shape (num_chunks,)\n",
    "\n",
    "    # Get causal weights (use minimum of both for stronger causality)\n",
    "    weights_vort = jax.lax.stop_gradient(jnp.exp(-causal_tol * (M @ loss_vort)))\n",
    "    weights_cont = jax.lax.stop_gradient(jnp.exp(-causal_tol * (M @ loss_cont)))\n",
    "    weights = jnp.minimum(weights_vort, weights_cont)\n",
    "\n",
    "    # Weighted loss\n",
    "    weighted_loss = jnp.mean(weights * loss_vort) + 100.0*jnp.mean(weights * loss_cont)\n",
    "\n",
    "    return weighted_loss, l_E_new\n",
    "\n",
    "\n",
    "# IC Loss for u, v, w (w is derived from u, v)\n",
    "def ic_loss(model, l_I_u, l_I_v, l_I_w, ic_collocs, u0_data, v0_data, w0_data):\n",
    "\n",
    "    # Get u, v predictions\n",
    "    uv_pred = model(ic_collocs)  # shape (batch_size, 2)\n",
    "    u_pred = uv_pred[:, 0:1]\n",
    "    v_pred = uv_pred[:, 1:2]\n",
    "    \n",
    "    # Get w prediction (derived from u, v)\n",
    "    w_pred = nst_w_func(model, ic_collocs)  # shape (batch_size, 1)\n",
    "\n",
    "    # Residuals\n",
    "    u_res = u_pred - u0_data\n",
    "    v_res = v_pred - v0_data\n",
    "    w_res = w_pred - w0_data\n",
    "\n",
    "    # RBA weights for u\n",
    "    abs_res_u = jnp.abs(u_res)\n",
    "    l_I_u_new = (RBA_gamma*l_I_u) + (RBA_eta*abs_res_u/jnp.max(abs_res_u))\n",
    "    \n",
    "    # RBA weights for v\n",
    "    abs_res_v = jnp.abs(v_res)\n",
    "    l_I_v_new = (RBA_gamma*l_I_v) + (RBA_eta*abs_res_v/jnp.max(abs_res_v))\n",
    "    \n",
    "    # RBA weights for w\n",
    "    abs_res_w = jnp.abs(w_res)\n",
    "    l_I_w_new = (RBA_gamma*l_I_w) + (RBA_eta*abs_res_w/jnp.max(abs_res_w))\n",
    "\n",
    "    # Weighted residuals\n",
    "    w_res_u = l_I_u_new * u_res\n",
    "    w_res_v = l_I_v_new * v_res\n",
    "    w_res_w = l_I_w_new * w_res\n",
    "\n",
    "    # Total IC loss\n",
    "    loss = jnp.mean(w_res_u**2) + jnp.mean(w_res_v**2) + jnp.mean(w_res_w**2)\n",
    "\n",
    "    return loss, (l_I_u_new, l_I_v_new, l_I_w_new)\n",
    "\n",
    "\n",
    "@nnx.jit\n",
    "def train_step(model, optimizer, collocs, ic_collocs, u0_data, v0_data, w0_data, \n",
    "               λ_E, λ_I, l_E, l_I_u, l_I_v, l_I_w):\n",
    "\n",
    "    # PDE loss\n",
    "    (loss_E, l_E_new), grads_E = nnx.value_and_grad(pde_loss, has_aux=True)(model, l_E, collocs)\n",
    "\n",
    "    # IC loss\n",
    "    (loss_I, (l_I_u_new, l_I_v_new, l_I_w_new)), grads_I = nnx.value_and_grad(ic_loss, has_aux=True)(\n",
    "        model, l_I_u, l_I_v, l_I_w, ic_collocs, u0_data, v0_data, w0_data\n",
    "    )\n",
    "    \n",
    "    # Compute total loss\n",
    "    loss = λ_E*loss_E + λ_I*loss_I\n",
    "\n",
    "    # Compute total gradients\n",
    "    grads = jax.tree_util.tree_map(lambda g1, g2: λ_E * g1 + λ_I * g2, grads_E, grads_I)\n",
    "\n",
    "    # Optimizer step\n",
    "    optimizer.update(grads)\n",
    "\n",
    "    return loss, grads_E, grads_I, l_E_new, l_I_u_new, l_I_v_new, l_I_w_new\n",
    "\n",
    "\n",
    "@nnx.jit\n",
    "def get_RAD_indices(model, collocs_pool, old_indices, l_E, l_E_pool):\n",
    "\n",
    "    # Apply updates from old indices to pool\n",
    "    updated_pool = l_E_pool.at[old_indices].set(l_E)\n",
    "\n",
    "    # Get full residuals\n",
    "    resids = pde_res(model, collocs_pool, Re=Re)  # shape (pool_size, 2)\n",
    "    \n",
    "    # Use mean of residuals for RAD\n",
    "    resids_mean = jnp.mean(resids**2, axis=1, keepdims=True)  # shape (pool_size, 1)\n",
    "    \n",
    "    # Multiply by RBA weights\n",
    "    w_resids = updated_pool * resids_mean\n",
    "    \n",
    "    # Get absolute\n",
    "    wa_resids = jnp.abs(w_resids)\n",
    "\n",
    "    # Raise to power rad_a\n",
    "    ea = jnp.power(wa_resids, rad_a)\n",
    "    \n",
    "    # Divide by mean and add rad_c\n",
    "    px = (ea/jnp.mean(ea)) + rad_c\n",
    "    \n",
    "    # Normalize\n",
    "    px_norm = (px / jnp.sum(px))[:,0]\n",
    "\n",
    "    sorted_indices = _get_colloc_indices(collocs_pool=collocs_pool, batch_size=batch_size, px=px_norm, seed=seed)\n",
    "\n",
    "    return sorted_indices, updated_pool\n",
    "\n",
    "\n",
    "def model_eval_nst(model, coords, u_ref, v_ref, w_ref):\n",
    "    \"\"\"Evaluate model and compute L² errors for u, v, w over full trajectory.\"\"\"\n",
    "    # Get predictions\n",
    "    uv_pred = model(coords)\n",
    "    u_pred = uv_pred[:, 0].reshape(u_ref.shape)\n",
    "    v_pred = uv_pred[:, 1].reshape(v_ref.shape)\n",
    "    w_pred = nst_w_func(model, coords).reshape(w_ref.shape)\n",
    "\n",
    "    # L² errors    \n",
    "\n",
    "    u_error = jnp.linalg.norm(u_pred - u_ref) / jnp.linalg.norm(u_ref)   \n",
    "    w_error = jnp.linalg.norm(w_pred - w_ref) / jnp.linalg.norm(w_ref)\n",
    "    v_error = jnp.linalg.norm(v_pred - v_ref) / jnp.linalg.norm(v_ref)\n",
    "\n",
    "    return u_error, v_error, w_error, u_pred, v_pred, w_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb23b19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a8a38e5",
   "metadata": {},
   "source": [
    "## Data & Grid-Search Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ac99c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom function for 3D collocation points (t, x, y) on torus\n",
    "def _get_pde_collocs_3d(t_range, x_range, y_range, sample_size_t, sample_size_xy):\n",
    "    \"\"\"Generate collocation points for 3D (t, x, y) domain.\"\"\"\n",
    "    t = jnp.linspace(t_range[0], t_range[1], sample_size_t)\n",
    "    x = jnp.linspace(x_range[0], x_range[1], sample_size_xy)\n",
    "    y = jnp.linspace(y_range[0], y_range[1], sample_size_xy)\n",
    "    T, X, Y = jnp.meshgrid(t, x, y, indexing='ij')\n",
    "    collocs_pool = jnp.stack([T.flatten(), X.flatten(), Y.flatten()], axis=1)\n",
    "    return collocs_pool\n",
    "\n",
    "\n",
    "def _get_ic_collocs_2d(x_range, y_range, sample_size):\n",
    "    \"\"\"Generate IC collocation points for 2D spatial domain at t=0.\"\"\"\n",
    "    t = jnp.array([0.0], dtype=float)\n",
    "    x = jnp.linspace(x_range[0], x_range[1], sample_size)\n",
    "    y = jnp.linspace(y_range[0], y_range[1], sample_size)\n",
    "    T, X, Y = jnp.meshgrid(t, x, y, indexing='ij')\n",
    "    ic_collocs = jnp.stack([T.flatten(), X.flatten(), Y.flatten()], axis=1)\n",
    "    return ic_collocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5288511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load reference data (new format with full u, v, w solutions)\n",
    "ref = np.load('data/ns.npy', allow_pickle=True).item()\n",
    "\n",
    "# Full reference solutions: shape (11, 64, 64)\n",
    "u_sol = jnp.array(ref['u'])\n",
    "v_sol = jnp.array(ref['v'])\n",
    "w_sol = jnp.array(ref['w'])\n",
    "\n",
    "# Coordinates\n",
    "t_ref = ref['t']  # shape (11,)\n",
    "x_ref = ref['x']  # shape (64,)\n",
    "y_ref = ref['y']  # shape (64,)\n",
    "\n",
    "# Initial conditions (64x64 grid)\n",
    "u0_ref = jnp.array(ref['u0'])\n",
    "v0_ref = jnp.array(ref['v0'])\n",
    "w0_ref = jnp.array(ref['w0'])\n",
    "\n",
    "# Reynolds number\n",
    "Re = 1.0 / ref['viscosity']  # Re = 100\n",
    "\n",
    "# Domain ranges\n",
    "t_max = float(t_ref.max())\n",
    "x_max = float(x_ref.max())\n",
    "y_max = float(y_ref.max())\n",
    "\n",
    "\n",
    "# Grid sizeprint(f\"Initial conditions shapes: u0={u0_ref.shape}, v0={v0_ref.shape}, w0={w0_ref.shape}\")\n",
    "\n",
    "N_t, N_x, N_y = w_sol.shape\n",
    "print(f\"Spatial grid: {N_x}x{N_y} = {N_x*N_y} IC points\")\n",
    "\n",
    "print(f\"Reference solution shapes: u={u_sol.shape}, v={v_sol.shape}, w={w_sol.shape}\")\n",
    "\n",
    "print(f\"Domain: t ∈ [0, {t_max:.4f}], x ∈ [0, {x_max:.4f}], y ∈ [0, {y_max:.4f}]\")\n",
    "print(f\"Reynolds number: Re = {Re:.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc62174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collocation points for PDE residual (t, x, y)\n",
    "collocs_pool = _get_pde_collocs_3d(t_range=(0, t_max), x_range=(0, x_max), y_range=(0, y_max), \n",
    "                                   sample_size_t=32, sample_size_xy=64)\n",
    "\n",
    "# IC collocation points at t=0 (use same grid as reference for IC)\n",
    "ic_x = jnp.array(x_ref)\n",
    "ic_y = jnp.array(y_ref)\n",
    "IC_X, IC_Y = jnp.meshgrid(ic_x, ic_y, indexing='ij')\n",
    "ic_collocs = jnp.stack([jnp.zeros_like(IC_X.flatten()), IC_X.flatten(), IC_Y.flatten()], axis=1)\n",
    "\n",
    "# IC data from reference\n",
    "u0_data = u0_ref.flatten().reshape(-1, 1)\n",
    "v0_data = v0_ref.flatten().reshape(-1, 1)\n",
    "w0_data = w0_ref.flatten().reshape(-1, 1)\n",
    "\n",
    "# Evaluation coordinates (full trajectory)\n",
    "T_eval, X_eval, Y_eval = jnp.meshgrid(jnp.array(t_ref), jnp.array(x_ref), jnp.array(y_ref), indexing='ij')\n",
    "coords = jnp.hstack([T_eval.flatten()[:, None], X_eval.flatten()[:, None], Y_eval.flatten()[:, None]])\n",
    "\n",
    "print(f\"Collocs pool shape: {collocs_pool.shape}\")\n",
    "print(f\"IC collocs shape: {ic_collocs.shape}\")\n",
    "print(f\"Coords shape: {coords.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9153eb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training epochs\n",
    "num_epochs = 100_000\n",
    "\n",
    "# Scheduler configurations\n",
    "learning_rate = 1e-3\n",
    "decay_steps = 2000\n",
    "decay_rate = 0.9\n",
    "warmup_steps = 1000\n",
    "\n",
    "# Define causal training parameters\n",
    "causal_tol = 1.0\n",
    "num_chunks = 32\n",
    "M = jnp.triu(jnp.ones((num_chunks, num_chunks)), k=1).T\n",
    "\n",
    "# Define Grad Norm parameters\n",
    "grad_mixing = 0.9\n",
    "f_grad_norm = 1000\n",
    "\n",
    "# Define resampling parameters\n",
    "batch_size = 2**12\n",
    "f_resample = 2000\n",
    "rad_a = 1.0\n",
    "rad_c = 1.0\n",
    "\n",
    "# Define RBA parameters\n",
    "RBA_gamma = 0.999\n",
    "RBA_eta = 0.01\n",
    "\n",
    "# Define model parameters\n",
    "n_in = 3  # (t, x, y)\n",
    "n_out = 2  # (u, v) - w is derived\n",
    "D = 5\n",
    "\n",
    "# Periodic embeddings for x and y (period = 2π ≈ 6.28)\n",
    "# Normalize by dividing domain by 2π\n",
    "period_axes = {1: 1.0, 2: 1.0}  # will use sin/cos embeddings\n",
    "\n",
    "sine_D = 5\n",
    "init_scheme = {'type': 'glorot', 'gain': None, 'norm_pow': 0, 'distribution': 'uniform', 'sample_size': 10000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3c9cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Architecture parameters\n",
    "archs = ['RGA KAN', 'PirateNet', 'cPIKAN']\n",
    "\n",
    "arch_params = {'RGA KAN' : {'n_hidden' : 16, 'num_blocks' : 6},\n",
    "               'PirateNet' : {'n_hidden' : 36, 'num_blocks' : 4},\n",
    "               'cPIKAN' : {'n_hidden' : 18, 'num_layers' : 12}}\n",
    "\n",
    "print(\"Expected to train models with following number of parameters:\")\n",
    "\n",
    "rga_width = arch_params['RGA KAN']['n_hidden']\n",
    "rga_blocks = arch_params['RGA KAN']['num_blocks']\n",
    "rga_params = count_rga(n_in, period_axes, n_out, rga_width, rga_blocks, D, sine_D)\n",
    "print(f\"RGA KAN: {rga_params} parameters\")\n",
    "\n",
    "pirate_width = arch_params['PirateNet']['n_hidden']\n",
    "pirate_blocks = arch_params['PirateNet']['num_blocks']\n",
    "pirate_params = count_pirate(n_in, period_axes, n_out, pirate_width, pirate_blocks)\n",
    "print(f\"PirateNet: {pirate_params} parameters\")\n",
    "\n",
    "pikan_width = arch_params['cPIKAN']['n_hidden']\n",
    "pikan_depth = arch_params['cPIKAN']['num_layers']\n",
    "pikan_params = count_pikan(n_in, period_axes, n_out, pikan_width, pikan_depth, D)\n",
    "print(f\"cPIKAN: {pikan_params} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d626a1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b9afe6d0",
   "metadata": {},
   "source": [
    "### cPIKAN / Pirate Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6082b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for arch in ['PirateNet', 'cPIKAN']:\n",
    "\n",
    "    RESULTS[arch] = dict()\n",
    "\n",
    "    n_hidden = arch_params[arch]['n_hidden']\n",
    "\n",
    "    if arch == 'PirateNet':\n",
    "        num_blocks = arch_params[arch]['num_blocks']\n",
    "        depth = int(3*num_blocks)\n",
    "    elif arch == 'cPIKAN':\n",
    "        num_layers = arch_params[arch]['num_layers']\n",
    "        depth = num_layers\n",
    "        \n",
    "    print(f\"Training {arch} with depth = {depth} and width = {n_hidden}.\")\n",
    "\n",
    "    for idx, run in enumerate([0, 7, 42]):\n",
    "\n",
    "        RESULTS[arch][idx] = dict()\n",
    "        \n",
    "        # Initialize RBA weights - full pool\n",
    "        l_E_pool = jnp.ones((collocs_pool.shape[0], 1))\n",
    "        # Also get RBAs for ICs (separate for u, v, w)\n",
    "        l_I_u = jnp.ones((ic_collocs.shape[0], 1))\n",
    "        l_I_v = jnp.ones((ic_collocs.shape[0], 1))\n",
    "        l_I_w = jnp.ones((ic_collocs.shape[0], 1))\n",
    "    \n",
    "        # Get starting collocation points & RBA weights\n",
    "        sorted_indices = _get_colloc_indices(collocs_pool=collocs_pool, batch_size=batch_size, px=None, seed=seed)\n",
    "        \n",
    "        collocs = collocs_pool[sorted_indices]\n",
    "        l_E = l_E_pool[sorted_indices]\n",
    "        \n",
    "        # Get opt_type\n",
    "        opt_type = _get_adam(learning_rate=learning_rate, decay_steps=decay_steps, decay_rate=decay_rate, warmup_steps=warmup_steps)\n",
    "\n",
    "        # Define model (no wrappers needed - periodic BCs handled via period_axes)\n",
    "        if arch == 'PirateNet':\n",
    "            model = PirateNet(n_in=n_in, n_out=n_out, n_hidden=n_hidden, num_blocks=num_blocks,\n",
    "                              alpha=0.0, ref=None, period_axes=period_axes, rff_std=1.0,\n",
    "                              RWF={\"mean\": 1.0, \"std\": 0.1}, seed=seed+run)\n",
    "        elif arch == 'cPIKAN':\n",
    "            model = KAN(n_in=n_in, n_out=n_out, n_hidden=n_hidden, num_layers=num_layers, D=D,\n",
    "                        init_scheme=init_scheme, period_axes=period_axes, rff_std=None,\n",
    "                        seed=seed+run)\n",
    "\n",
    "        if idx == 0:\n",
    "            print(f\"Initialized model with {count_params(model)} parameters.\")\n",
    "        \n",
    "        # Define global loss weights\n",
    "        λ_E = jnp.array(1.0, dtype=float)\n",
    "        λ_I = jnp.array(1e5, dtype=float)\n",
    "\n",
    "        # Set optimizer\n",
    "        optimizer = nnx.Optimizer(model, opt_type)\n",
    "\n",
    "        tick = time.time()\n",
    "    \n",
    "        # Start training\n",
    "        for epoch in range(num_epochs):\n",
    "        \n",
    "            loss, grads_E, grads_I, l_E, l_I_u, l_I_v, l_I_w = train_step(\n",
    "                model, optimizer, collocs, ic_collocs, u0_data, v0_data, w0_data, \n",
    "                λ_E, λ_I, l_E, l_I_u, l_I_v, l_I_w\n",
    "            )\n",
    "            \n",
    "            # Perform grad norm\n",
    "            if (epoch != 0) and (epoch % f_grad_norm == 0):\n",
    "        \n",
    "                λ_Ε, λ_I = grad_norm(grads_E, grads_I, λ_E, λ_I, grad_mixing)\n",
    "        \n",
    "            # Perform RAD\n",
    "            if (epoch != 0) and (epoch % f_resample == 0):\n",
    "    \n",
    "                # Get new indices after resampling\n",
    "                sorted_indices, l_E_pool = get_RAD_indices(model, collocs_pool, sorted_indices, l_E, l_E_pool)\n",
    "                # Set new batch of collocs and l_E\n",
    "                collocs = collocs_pool[sorted_indices]\n",
    "                l_E = l_E_pool[sorted_indices]\n",
    "                \n",
    "\n",
    "        tack = time.time()\n",
    "        \n",
    "        # Evaluate on full trajectory\n",
    "        u_err, v_err, w_err, u_pred, v_pred, w_pred = model_eval_nst(model, coords, u_sol, v_sol, w_sol)\n",
    "\n",
    "        print(f\"\\tRun = {idx}\\t L²(u)={u_err:.2e}\\t L²(v)={v_err:.2e}\\t L²(w)={w_err:.2e}\\t Loss = {loss:.2e}\\t Time = {(tack-tick)/60:.2f} mins\")\n",
    "\n",
    "        RESULTS[arch][idx]['l2_u'] = np.asarray(device_get(u_err))\n",
    "        RESULTS[arch][idx]['l2_v'] = np.asarray(device_get(v_err))\n",
    "        RESULTS[arch][idx]['l2_w'] = np.asarray(device_get(w_err))\n",
    "\n",
    "        RESULTS[arch][idx]['loss'] = np.asarray(device_get(loss))       \n",
    "        RESULTS[arch][idx]['time'] = (tack-tick)/num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e216e84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36453c97",
   "metadata": {},
   "source": [
    "### RGA Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b582bd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_blocks = arch_params['RGA KAN']['num_blocks']\n",
    "n_hidden = arch_params['RGA KAN']['n_hidden']\n",
    "depth = int(2*num_blocks)\n",
    "\n",
    "print(f\"Training RGA KAN with depth = {depth} and width = {n_hidden}.\")\n",
    "\n",
    "for alpha in [0.0, 1.0]:\n",
    "    for beta in [0.0, 1.0]:\n",
    "\n",
    "        RESULTS[(alpha,beta)] = dict()\n",
    "        print(f\"Training for alpha = {alpha} and beta = {beta}.\")\n",
    "    \n",
    "        for idx, run in enumerate([0, 7, 42]): \n",
    "    \n",
    "            RESULTS[(alpha,beta)][idx] = dict()\n",
    "            \n",
    "            # Initialize RBA weights - full pool\n",
    "            l_E_pool = jnp.ones((collocs_pool.shape[0], 1))\n",
    "            # Also get RBAs for ICs (separate for u, v, w)\n",
    "            l_I_u = jnp.ones((ic_collocs.shape[0], 1))\n",
    "            l_I_v = jnp.ones((ic_collocs.shape[0], 1))\n",
    "            l_I_w = jnp.ones((ic_collocs.shape[0], 1))\n",
    "        \n",
    "            # Get starting collocation points & RBA weights\n",
    "            sorted_indices = _get_colloc_indices(collocs_pool=collocs_pool, batch_size=batch_size, px=None, seed=seed)\n",
    "            \n",
    "            collocs = collocs_pool[sorted_indices]\n",
    "            l_E = l_E_pool[sorted_indices]\n",
    "            \n",
    "            # Get opt_type\n",
    "            opt_type = _get_adam(learning_rate=learning_rate, decay_steps=decay_steps, decay_rate=decay_rate, warmup_steps=warmup_steps)\n",
    "    \n",
    "            # Define model (no wrappers needed - periodic BCs handled via period_axes)\n",
    "            model = RGAKAN(n_in=n_in, n_out=n_out, n_hidden=n_hidden, num_blocks=num_blocks, D=D,\n",
    "                           init_scheme=init_scheme, alpha=alpha, beta=beta, ref=None,\n",
    "                           period_axes=period_axes, rff_std=None, sine_D=sine_D, seed=seed+run)\n",
    "    \n",
    "            if idx == 0:\n",
    "                print(f\"Initialized model with {count_params(model)} parameters.\")\n",
    "            \n",
    "            # Define global loss weights\n",
    "            λ_E = jnp.array(1.0, dtype=float)\n",
    "            λ_I = jnp.array(1e5, dtype=float)\n",
    "    \n",
    "            # Set optimizer\n",
    "            optimizer = nnx.Optimizer(model, opt_type)\n",
    "    \n",
    "            tick = time.time()\n",
    "        \n",
    "            # Start training\n",
    "            for epoch in range(num_epochs):\n",
    "            \n",
    "                loss, grads_E, grads_I, l_E, l_I_u, l_I_v, l_I_w = train_step(\n",
    "                    model, optimizer, collocs, ic_collocs, u0_data, v0_data, w0_data, \n",
    "                    λ_E, λ_I, l_E, l_I_u, l_I_v, l_I_w\n",
    "                )\n",
    "                \n",
    "                # Perform grad norm\n",
    "                if (epoch != 0) and (epoch % f_grad_norm == 0):\n",
    "            \n",
    "                    λ_Ε, λ_I = grad_norm(grads_E, grads_I, λ_E, λ_I, grad_mixing)\n",
    "            \n",
    "                # Perform RAD\n",
    "                if (epoch != 0) and (epoch % f_resample == 0):\n",
    "        \n",
    "                    # Get new indices after resampling\n",
    "                    sorted_indices, l_E_pool = get_RAD_indices(model, collocs_pool, sorted_indices, l_E, l_E_pool)\n",
    "                    # Set new batch of collocs and l_E\n",
    "                    collocs = collocs_pool[sorted_indices]\n",
    "                    l_E = l_E_pool[sorted_indices]\n",
    "    \n",
    "            tack = time.time()\n",
    "            \n",
    "            # Evaluate on full trajectory\n",
    "            u_err, v_err, w_err, u_pred, v_pred, w_pred = model_eval_nst(model, coords, u_sol, v_sol, w_sol)\n",
    "    \n",
    "            print(f\"\\tRun = {idx}\\t L²(u)={u_err:.2e}\\t L²(v)={v_err:.2e}\\t L²(w)={w_err:.2e}\\t Loss = {loss:.2e}\\t Time = {(tack-tick)/60:.2f} mins\")\n",
    "    \n",
    "            RESULTS[(alpha,beta)][idx]['l2_u'] = np.asarray(device_get(u_err))\n",
    "            RESULTS[(alpha,beta)][idx]['l2_v'] = np.asarray(device_get(v_err))\n",
    "            RESULTS[(alpha,beta)][idx]['l2_w'] = np.asarray(device_get(w_err))\n",
    "            RESULTS[(alpha,beta)][idx]['loss'] = np.asarray(device_get(loss))\n",
    "            RESULTS[(alpha,beta)][idx]['time'] = (tack-tick)/num_epochs\n",
    "\n",
    "            RESULTS[(alpha,beta)][idx]['u_pred'] = np.asarray(device_get(u_pred))\n",
    "            RESULTS[(alpha,beta)][idx]['w_pred'] = np.asarray(device_get(w_pred))\n",
    "            RESULTS[(alpha,beta)][idx]['v_pred'] = np.asarray(device_get(v_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b52a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(result_file, \"wb\") as f:\n",
    "    pickle.dump(RESULTS, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dad198b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "46634d73",
   "metadata": {},
   "source": [
    "## Plots & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf775a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(result_file, \"rb\") as f:\n",
    "    RESULTS = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2af42be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_stats(RESULTS, model_idx, metric='l2'):\n",
    "    vals = []\n",
    "    runs = RESULTS.get(model_idx, {})\n",
    "    for i in (0, 1, 2):\n",
    "        try:\n",
    "            v = runs[i][metric]\n",
    "        except (KeyError, TypeError):\n",
    "            continue\n",
    "        v = np.array(v, dtype=float).squeeze()\n",
    "        vals.append(float(v))\n",
    "\n",
    "    if len(vals) == 0:\n",
    "        return np.nan, np.nan\n",
    "    if len(vals) == 1:\n",
    "        return float(vals[0]), np.nan\n",
    "\n",
    "    mean = float(np.mean(vals))\n",
    "    se = float(np.std(vals, ddof=1) / np.sqrt(len(vals)))\n",
    "    return mean, se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87da31ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RESULTS - L² Errors\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "def print_model_stats(name, key):\n",
    "    mu, su = metric_stats(RESULTS, key, 'l2_u')\n",
    "    mv, sv = metric_stats(RESULTS, key, 'l2_v')\n",
    "    mw, sw = metric_stats(RESULTS, key, 'l2_w')\n",
    "    tt, _ = metric_stats(RESULTS, key, 'time')\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  L²(u) = {mu:.3e} ± {su:.3e}\")\n",
    "    print(f\"  L²(v) = {mv:.3e} ± {sv:.3e}\")\n",
    "    print(f\"  L²(w) = {mw:.3e} ± {sw:.3e}\")\n",
    "    print(f\"  Time  = {tt*1000:.2f} ms/epoch\")\n",
    "    print()\n",
    "\n",
    "print_model_stats(\"PirateNet\", 'PirateNet')\n",
    "print_model_stats(\"cPIKAN\", 'cPIKAN')\n",
    "print_model_stats(\"RGAKAN (α=0, β=0)\", (0,0))\n",
    "print_model_stats(\"RGAKAN (α=1, β=0)\", (1,0))\n",
    "print_model_stats(\"RGAKAN (α=0, β=1)\", (0,1))\n",
    "print_model_stats(\"RGAKAN (α=1, β=1)\", (1,1))\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"\\nFinal Losses:\")\n",
    "print(\"-\" * 50)\n",
    "m, s = metric_stats(RESULTS, 'PirateNet', 'loss')\n",
    "print(f\"PirateNet:\\t\\t Loss = {m:.3e} ± {s:.3e}\")\n",
    "m, s = metric_stats(RESULTS, 'cPIKAN', 'loss')\n",
    "print(f\"cPIKAN:\\t\\t\\t Loss = {m:.3e} ± {s:.3e}\")\n",
    "m, s = metric_stats(RESULTS, (0,0), 'loss')\n",
    "print(f\"RGAKAN (α=0, β=0):\\t Loss = {m:.3e} ± {s:.3e}\")\n",
    "m, s = metric_stats(RESULTS, (1,0), 'loss')\n",
    "print(f\"RGAKAN (α=1, β=0):\\t Loss = {m:.3e} ± {s:.3e}\")\n",
    "\n",
    "m, s = metric_stats(RESULTS, (0,1), 'loss')\n",
    "print(f\"RGAKAN (α=1, β=1):\\t Loss = {m:.3e} ± {s:.3e}\")\n",
    "\n",
    "print(f\"RGAKAN (α=0, β=1):\\t Loss = {m:.3e} ± {s:.3e}\")\n",
    "m, s = metric_stats(RESULTS, (1,1), 'loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40972d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0841d1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from matplotlib import ticker\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "LABEL_FS = 16\n",
    "TITLE_FS = 18\n",
    "TICK_FS = 14\n",
    "CBAR_FS = 16\n",
    "plots_dir = \"plots\"\n",
    "os.makedirs(plots_dir, exist_ok=True)\n",
    "\n",
    "ref = np.load('data/ns.npy', allow_pickle=True).item()\n",
    "u_ref = np.array(ref['u'])\n",
    "v_ref = np.array(ref['v'])\n",
    "w_ref = np.array(ref['w'])\n",
    "x_ref = ref['x']\n",
    "y_ref = ref['y']\n",
    "\n",
    "# --- Plotting Function ---\n",
    "def plot_nst_grid(u_pred, v_pred, w_pred, u_ref, v_ref, w_ref, x, y, save_fig=False):\n",
    "    \n",
    "    # Select last time step (t=0.1)\n",
    "    t_idx = -1\n",
    "    \n",
    "    # Prepare data for rows: u, v, w\n",
    "    preds = [u_pred[t_idx], v_pred[t_idx], w_pred[t_idx]]\n",
    "    refs = [u_ref[t_idx], v_ref[t_idx], w_ref[t_idx]]\n",
    "    \n",
    "    nrows = 3\n",
    "    extent = [np.min(x), np.max(x), np.min(y), np.max(y)]\n",
    "    cmap = sns.color_palette(\"Spectral\", as_cmap=True)\n",
    "    \n",
    "    fig, axs = plt.subplots(nrows, 3, figsize=(14, 2.5*nrows), constrained_layout=False)\n",
    "    \n",
    "    col_titles = [\"Reference\", \"Prediction\", \"Absolute Error\"]\n",
    "    row_titles = [r\"$u$\", r\"$v$\", r\"$w$\"]\n",
    "    var_names = [\"u\", \"v\", \"w\"]\n",
    "\n",
    "    for i in range(nrows):\n",
    "        ref = refs[i]\n",
    "        pred = preds[i]\n",
    "        diff = np.abs(pred - ref)\n",
    "        \n",
    "        panels = [ref.T, pred.T, diff.T]\n",
    "        \n",
    "        # Determine clims for this row\n",
    "        clim_val = (np.min(ref), np.max(ref))\n",
    "        clim_diff = (0, np.max(diff))\n",
    "\n",
    "        for j in range(3):\n",
    "            ax = axs[i, j]\n",
    "            img = ax.imshow(panels[j], origin='lower', aspect='auto', extent=extent, cmap=cmap)\n",
    "\n",
    "            # Column titles\n",
    "            if i == 0:\n",
    "                ax.set_title(col_titles[j], fontsize=TITLE_FS)\n",
    "\n",
    "            # Row titles\n",
    "            \"\"\"\n",
    "            if j == 2:\n",
    "                ax.annotate(\n",
    "                    row_titles[i],\n",
    "                    xy=(1.70, 0.5), xycoords='axes fraction',\n",
    "                    ha='left', va='center',\n",
    "                    rotation=0, # Horizontal for variable names usually looks better, or 90\n",
    "                    fontsize=TITLE_FS\n",
    "                )\"\"\"\n",
    "\n",
    "            # Axis labels\n",
    "            if i == nrows - 1:\n",
    "                ax.set_xlabel(r\"$x$\", fontsize=LABEL_FS)\n",
    "            else:\n",
    "                ax.set_xlabel(\"\")\n",
    "            if j == 0:\n",
    "                ax.set_ylabel(r\"$y$\", fontsize=LABEL_FS)\n",
    "            else:\n",
    "                ax.set_ylabel(\"\")\n",
    "    \n",
    "            ax.tick_params(axis='both', which='major', labelsize=TICK_FS)\n",
    "\n",
    "            # Apply clims\n",
    "            if j < 2:\n",
    "                img.set_clim(*clim_val)\n",
    "            else:\n",
    "                img.set_clim(*clim_diff)\n",
    "\n",
    "            # Colorbar\n",
    "            divider = make_axes_locatable(ax)\n",
    "            cax = divider.append_axes(\"right\", size=\"5%\", pad=0.15)\n",
    "            cbar = fig.colorbar(img, cax=cax, orientation='vertical')\n",
    "            \n",
    "            # Formatter\n",
    "            cbar.formatter = ticker.ScalarFormatter(useMathText=True)\n",
    "            cbar.formatter.set_scientific(True)\n",
    "            cbar.formatter.set_powerlimits((0, 0))\n",
    "            cbar.formatter.set_useOffset(False)\n",
    "            cbar.update_ticks()\n",
    "            \n",
    "            offset_text = cbar.ax.yaxis.get_offset_text()\n",
    "            offset_text.set_fontsize(TICK_FS)\n",
    "            offset_text.set_x(7.5)\n",
    "\n",
    "            cbar.ax.tick_params(labelsize=TICK_FS)\n",
    "            \n",
    "            # Label construction\n",
    "            if j == 0:\n",
    "                lbl = fr\"${var_names[i]}_{{\\mathrm{{ref}}}}$\"\n",
    "            elif j == 1:\n",
    "                lbl = fr\"${var_names[i]}_{{\\mathrm{{pred}}}}$\"\n",
    "            else:\n",
    "                lbl = fr\"$|{var_names[i]}_{{\\mathrm{{pred}}}}-{var_names[i]}_{{\\mathrm{{ref}}}}|$\"\n",
    "            \n",
    "            cbar.set_label(lbl, fontsize=LABEL_FS, labelpad=10)\n",
    "\n",
    "    plt.subplots_adjust(left=0.12, wspace=0.55, hspace=0.3, bottom=0.0)\n",
    "\n",
    "    if save_fig:\n",
    "        plt.savefig(f\"{plots_dir}/final_nst.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73585ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_data = RESULTS[(1,0)][0]\n",
    "u_pred_full = run_data['u_pred'].reshape(u_ref.shape)\n",
    "v_pred_full = run_data['v_pred'].reshape(v_ref.shape)\n",
    "w_pred_full = run_data['w_pred'].reshape(w_ref.shape)\n",
    "\n",
    "# --- Run Plotting ---\n",
    "plot_nst_grid(u_pred_full, v_pred_full, w_pred_full, u_ref, v_ref, w_ref, x_ref, y_ref, save_fig=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fbb141-f81a-4d8e-ac65-e6e4c8c2f64d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
