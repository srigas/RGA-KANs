{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6edfedd0-18c5-46b8-9452-d1681c65eb39",
   "metadata": {},
   "source": [
    "# Experiment 14.2: Navier-Stokes Ablations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771e03f6-637d-4c88-bc6c-35a359422b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "jax.config.update(\"jax_default_matmul_precision\", \"highest\")\n",
    "\n",
    "from src.equations import nst_res as pde_res\n",
    "from src.equations import nst_w_func\n",
    "\n",
    "from src.utils import _get_adam, count_params, _get_colloc_indices, grad_norm\n",
    "from src.utils import count_rga, count_pirate, count_pikan\n",
    "from src.rgakan import RGAKAN\n",
    "from src.kan import KAN\n",
    "from src.piratenet import PirateNet\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "from jax import device_get\n",
    "\n",
    "import optax\n",
    "from flax import nnx\n",
    "\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import os\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "results_dir = \"results\"\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "result_file = os.path.join(results_dir, \"ablations_nst.pkl\")\n",
    "\n",
    "plots_dir = \"plots\"\n",
    "os.makedirs(plots_dir, exist_ok=True)\n",
    "\n",
    "RESULTS = dict()\n",
    "        \n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6dc5fc-7102-47cb-be28-34b038cf0a3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b8c61c8-b081-42c0-9603-f8df718c3687",
   "metadata": {},
   "source": [
    "## Basic Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b4684d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDE Loss - returns 2 residuals: vorticity transport and continuity\n",
    "def pde_loss(model, l_E, collocs, use_rba, use_causal, cont_weight):\n",
    "\n",
    "    residuals = pde_res(model, collocs, Re=Re)  # shape (batch_size, 2): [vorticity, continuity]\n",
    "\n",
    "    if use_rba:\n",
    "        # Get new RBA weights (use mean of residuals for weighting)\n",
    "        abs_res = jnp.abs(residuals).mean(axis=1, keepdims=True)  # shape (batch_size, 1)\n",
    "        l_E_new = (RBA_gamma*l_E) + (RBA_eta*abs_res/jnp.max(abs_res))\n",
    "        # Multiply by RBA weights\n",
    "        w_resids = l_E_new * residuals  # shape (batch_size, 2)\n",
    "    else:\n",
    "        l_E_new = l_E\n",
    "        w_resids = residuals\n",
    "\n",
    "    if use_causal:\n",
    "        # Reshape residuals for causal training (along time dimension)\n",
    "        # Shape: (num_chunks, points_per_chunk, 2)\n",
    "        resids_chunked = w_resids.reshape(num_chunks, -1, 2)\n",
    "\n",
    "        # Get average loss per chunk for each residual type\n",
    "        loss_vort = jnp.mean(resids_chunked[:, :, 0]**2, axis=1)  # shape (num_chunks,)\n",
    "        loss_cont = jnp.mean(resids_chunked[:, :, 1]**2, axis=1)  # shape (num_chunks,)\n",
    "\n",
    "        # Get causal weights (use minimum of both for stronger causality)\n",
    "        weights_vort = jax.lax.stop_gradient(jnp.exp(-causal_tol * (M @ loss_vort)))\n",
    "        weights_cont = jax.lax.stop_gradient(jnp.exp(-causal_tol * (M @ loss_cont)))\n",
    "        weights = jnp.minimum(weights_vort, weights_cont)\n",
    "\n",
    "        # Weighted loss\n",
    "        weighted_loss = jnp.mean(weights * loss_vort) + cont_weight*jnp.mean(weights * loss_cont)\n",
    "    else:\n",
    "        # Standard loss\n",
    "        weighted_loss = jnp.mean(w_resids[:, 0]**2) + cont_weight*jnp.mean(w_resids[:, 1]**2)\n",
    "\n",
    "    return weighted_loss, l_E_new\n",
    "\n",
    "\n",
    "# IC Loss for u, v, w (w is derived from u, v)\n",
    "def ic_loss(model, l_I_u, l_I_v, l_I_w, ic_collocs, u0_data, v0_data, w0_data, use_rba):\n",
    "\n",
    "    # Get u, v predictions\n",
    "    uv_pred = model(ic_collocs)  # shape (batch_size, 2)\n",
    "    u_pred = uv_pred[:, 0:1]\n",
    "    v_pred = uv_pred[:, 1:2]\n",
    "    \n",
    "    # Get w prediction (derived from u, v)\n",
    "    w_pred = nst_w_func(model, ic_collocs)  # shape (batch_size, 1)\n",
    "\n",
    "    # Residuals\n",
    "    u_res = u_pred - u0_data\n",
    "    v_res = v_pred - v0_data\n",
    "    w_res = w_pred - w0_data\n",
    "\n",
    "    if use_rba:\n",
    "        # RBA weights for u\n",
    "        abs_res_u = jnp.abs(u_res)\n",
    "        l_I_u_new = (RBA_gamma*l_I_u) + (RBA_eta*abs_res_u/jnp.max(abs_res_u))\n",
    "        \n",
    "        # RBA weights for v\n",
    "        abs_res_v = jnp.abs(v_res)\n",
    "        l_I_v_new = (RBA_gamma*l_I_v) + (RBA_eta*abs_res_v/jnp.max(abs_res_v))\n",
    "        \n",
    "        # RBA weights for w\n",
    "        abs_res_w = jnp.abs(w_res)\n",
    "        l_I_w_new = (RBA_gamma*l_I_w) + (RBA_eta*abs_res_w/jnp.max(abs_res_w))\n",
    "\n",
    "        # Weighted residuals\n",
    "        w_res_u = l_I_u_new * u_res\n",
    "        w_res_v = l_I_v_new * v_res\n",
    "        w_res_w = l_I_w_new * w_res\n",
    "    else:\n",
    "        l_I_u_new, l_I_v_new, l_I_w_new = l_I_u, l_I_v, l_I_w\n",
    "        w_res_u, w_res_v, w_res_w = u_res, v_res, w_res\n",
    "\n",
    "    # Total IC loss\n",
    "    loss = jnp.mean(w_res_u**2) + jnp.mean(w_res_v**2) + jnp.mean(w_res_w**2)\n",
    "\n",
    "    return loss, (l_I_u_new, l_I_v_new, l_I_w_new)\n",
    "\n",
    "\n",
    "@partial(nnx.jit, static_argnums=(7, 8))\n",
    "def train_step(model, optimizer, collocs, ic_collocs, u0_data, v0_data, w0_data, \n",
    "               use_rba, use_causal, cont_weight,\n",
    "               λ_E, λ_I, l_E, l_I_u, l_I_v, l_I_w):\n",
    "\n",
    "    # PDE loss\n",
    "    (loss_E, l_E_new), grads_E = nnx.value_and_grad(pde_loss, has_aux=True)(model, l_E, collocs, use_rba, use_causal, cont_weight)\n",
    "\n",
    "    # IC loss\n",
    "    (loss_I, (l_I_u_new, l_I_v_new, l_I_w_new)), grads_I = nnx.value_and_grad(ic_loss, has_aux=True)(\n",
    "        model, l_I_u, l_I_v, l_I_w, ic_collocs, u0_data, v0_data, w0_data, use_rba\n",
    "    )\n",
    "    \n",
    "    # Compute total loss\n",
    "    loss = λ_E*loss_E + λ_I*loss_I\n",
    "\n",
    "    # Compute total gradients\n",
    "    grads = jax.tree_util.tree_map(lambda g1, g2: λ_E * g1 + λ_I * g2, grads_E, grads_I)\n",
    "\n",
    "    # Optimizer step\n",
    "    optimizer.update(grads)\n",
    "\n",
    "    return loss, grads_E, grads_I, l_E_new, l_I_u_new, l_I_v_new, l_I_w_new\n",
    "\n",
    "\n",
    "@nnx.jit\n",
    "def get_RAD_indices(model, collocs_pool, old_indices, l_E, l_E_pool):\n",
    "\n",
    "    # Apply updates from old indices to pool\n",
    "    updated_pool = l_E_pool.at[old_indices].set(l_E)\n",
    "\n",
    "    # Get full residuals\n",
    "    resids = pde_res(model, collocs_pool, Re=Re)  # shape (pool_size, 2)\n",
    "    \n",
    "    # Use mean of residuals for RAD\n",
    "    resids_mean = jnp.mean(resids**2, axis=1, keepdims=True)  # shape (pool_size, 1)\n",
    "    \n",
    "    # Multiply by RBA weights\n",
    "    w_resids = updated_pool * resids_mean\n",
    "    \n",
    "    # Get absolute\n",
    "    wa_resids = jnp.abs(w_resids)\n",
    "\n",
    "    # Raise to power rad_a\n",
    "    ea = jnp.power(wa_resids, rad_a)\n",
    "    \n",
    "    # Divide by mean and add rad_c\n",
    "    px = (ea/jnp.mean(ea)) + rad_c\n",
    "    \n",
    "    # Normalize\n",
    "    px_norm = (px / jnp.sum(px))[:,0]\n",
    "\n",
    "    sorted_indices = _get_colloc_indices(collocs_pool=collocs_pool, batch_size=batch_size, px=px_norm, seed=seed)\n",
    "\n",
    "    return sorted_indices, updated_pool\n",
    "\n",
    "\n",
    "def model_eval_nst(model, coords, u_ref, v_ref, w_ref):\n",
    "    \"\"\"Evaluate model and compute L² errors for u, v, w over full trajectory.\"\"\"\n",
    "    # Get predictions\n",
    "    uv_pred = model(coords)\n",
    "    u_pred = uv_pred[:, 0].reshape(u_ref.shape)\n",
    "    v_pred = uv_pred[:, 1].reshape(v_ref.shape)\n",
    "    w_pred = nst_w_func(model, coords).reshape(w_ref.shape)\n",
    "\n",
    "    # L² errors    \n",
    "\n",
    "    u_error = jnp.linalg.norm(u_pred - u_ref) / jnp.linalg.norm(u_ref)   \n",
    "    w_error = jnp.linalg.norm(w_pred - w_ref) / jnp.linalg.norm(w_ref)\n",
    "    v_error = jnp.linalg.norm(v_pred - v_ref) / jnp.linalg.norm(v_ref)\n",
    "\n",
    "    return u_error, v_error, w_error, u_pred, v_pred, w_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25d302d",
   "metadata": {},
   "source": [
    "## Data & Grid-Search Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d97d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom function for 3D collocation points (t, x, y) on torus\n",
    "def _get_pde_collocs_3d(t_range, x_range, y_range, sample_size_t, sample_size_xy):\n",
    "    \"\"\"Generate collocation points for 3D (t, x, y) domain.\"\"\"\n",
    "    t = jnp.linspace(t_range[0], t_range[1], sample_size_t)\n",
    "    x = jnp.linspace(x_range[0], x_range[1], sample_size_xy)\n",
    "    y = jnp.linspace(y_range[0], y_range[1], sample_size_xy)\n",
    "    T, X, Y = jnp.meshgrid(t, x, y, indexing='ij')\n",
    "    collocs_pool = jnp.stack([T.flatten(), X.flatten(), Y.flatten()], axis=1)\n",
    "    return collocs_pool\n",
    "\n",
    "\n",
    "def _get_ic_collocs_2d(x_range, y_range, sample_size):\n",
    "    \"\"\"Generate IC collocation points for 2D spatial domain at t=0.\"\"\"\n",
    "    t = jnp.array([0.0], dtype=float)\n",
    "    x = jnp.linspace(x_range[0], x_range[1], sample_size)\n",
    "    y = jnp.linspace(y_range[0], y_range[1], sample_size)\n",
    "    T, X, Y = jnp.meshgrid(t, x, y, indexing='ij')\n",
    "    ic_collocs = jnp.stack([T.flatten(), X.flatten(), Y.flatten()], axis=1)\n",
    "    return ic_collocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338aeea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load reference data (new format with full u, v, w solutions)\n",
    "ref = np.load('data/ns.npy', allow_pickle=True).item()\n",
    "\n",
    "# Full reference solutions: shape (11, 64, 64)\n",
    "u_sol = jnp.array(ref['u'])\n",
    "v_sol = jnp.array(ref['v'])\n",
    "w_sol = jnp.array(ref['w'])\n",
    "\n",
    "# Coordinates\n",
    "t_ref = ref['t']  # shape (11,)\n",
    "x_ref = ref['x']  # shape (64,)\n",
    "y_ref = ref['y']  # shape (64,)\n",
    "\n",
    "# Initial conditions (64x64 grid)\n",
    "u0_ref = jnp.array(ref['u0'])\n",
    "v0_ref = jnp.array(ref['v0'])\n",
    "w0_ref = jnp.array(ref['w0'])\n",
    "\n",
    "# Reynolds number\n",
    "Re = 1.0 / ref['viscosity']  # Re = 100\n",
    "\n",
    "# Domain ranges\n",
    "t_max = float(t_ref.max())\n",
    "x_max = float(x_ref.max())\n",
    "y_max = float(y_ref.max())\n",
    "\n",
    "\n",
    "# Grid sizeprint(f\"Initial conditions shapes: u0={u0_ref.shape}, v0={v0_ref.shape}, w0={w0_ref.shape}\")\n",
    "\n",
    "N_t, N_x, N_y = w_sol.shape\n",
    "print(f\"Spatial grid: {N_x}x{N_y} = {N_x*N_y} IC points\")\n",
    "\n",
    "print(f\"Reference solution shapes: u={u_sol.shape}, v={v_sol.shape}, w={w_sol.shape}\")\n",
    "\n",
    "print(f\"Domain: t ∈ [0, {t_max:.4f}], x ∈ [0, {x_max:.4f}], y ∈ [0, {y_max:.4f}]\")\n",
    "print(f\"Reynolds number: Re = {Re:.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a263801c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collocation points for PDE residual (t, x, y)\n",
    "collocs_pool = _get_pde_collocs_3d(t_range=(0, t_max), x_range=(0, x_max), y_range=(0, y_max), \n",
    "                                   sample_size_t=32, sample_size_xy=64)\n",
    "\n",
    "# IC collocation points at t=0 (use same grid as reference for IC)\n",
    "ic_x = jnp.array(x_ref)\n",
    "ic_y = jnp.array(y_ref)\n",
    "IC_X, IC_Y = jnp.meshgrid(ic_x, ic_y, indexing='ij')\n",
    "ic_collocs = jnp.stack([jnp.zeros_like(IC_X.flatten()), IC_X.flatten(), IC_Y.flatten()], axis=1)\n",
    "\n",
    "# IC data from reference\n",
    "u0_data = u0_ref.flatten().reshape(-1, 1)\n",
    "v0_data = v0_ref.flatten().reshape(-1, 1)\n",
    "w0_data = w0_ref.flatten().reshape(-1, 1)\n",
    "\n",
    "# Evaluation coordinates (full trajectory)\n",
    "T_eval, X_eval, Y_eval = jnp.meshgrid(jnp.array(t_ref), jnp.array(x_ref), jnp.array(y_ref), indexing='ij')\n",
    "coords = jnp.hstack([T_eval.flatten()[:, None], X_eval.flatten()[:, None], Y_eval.flatten()[:, None]])\n",
    "\n",
    "print(f\"Collocs pool shape: {collocs_pool.shape}\")\n",
    "print(f\"IC collocs shape: {ic_collocs.shape}\")\n",
    "print(f\"Coords shape: {coords.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0a51f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training epochs\n",
    "num_epochs = 100_000\n",
    "\n",
    "# Scheduler configurations\n",
    "learning_rate = 1e-3\n",
    "decay_steps = 2000\n",
    "decay_rate = 0.9\n",
    "warmup_steps = 1000\n",
    "\n",
    "# Define causal training parameters\n",
    "causal_tol = 1.0\n",
    "num_chunks = 32\n",
    "M = jnp.triu(jnp.ones((num_chunks, num_chunks)), k=1).T\n",
    "\n",
    "# Define Grad Norm parameters\n",
    "grad_mixing = 0.9\n",
    "f_grad_norm = 1000\n",
    "\n",
    "# Define resampling parameters\n",
    "batch_size = 2**12\n",
    "f_resample = 2000\n",
    "rad_a = 1.0\n",
    "rad_c = 1.0\n",
    "\n",
    "# Define RBA parameters\n",
    "RBA_gamma = 0.999\n",
    "RBA_eta = 0.01\n",
    "\n",
    "# Define model parameters\n",
    "n_in = 3  # (t, x, y)\n",
    "n_out = 2  # (u, v) - w is derived\n",
    "D = 5\n",
    "\n",
    "# Periodic embeddings for x and y (period = 2π ≈ 6.28)\n",
    "# Normalize by dividing domain by 2π\n",
    "period_axes = {1: 1.0, 2: 1.0}  # will use sin/cos embeddings\n",
    "\n",
    "sine_D = 5\n",
    "init_scheme = {'type': 'glorot', 'gain': None, 'norm_pow': 0, 'distribution': 'uniform', 'sample_size': 10000}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d830f314",
   "metadata": {},
   "source": [
    "### Ablation Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e17fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cases = [\n",
    "    {'name': 'No Custom Weights', 'rba': False, 'rad': False, 'causal': False, 'grad_norm': False, 'lambda_I': 1.0, 'cont_weight': 1.0},\n",
    "    {'name': 'Only RBA', 'rba': True, 'rad': False, 'causal': False, 'grad_norm': False, 'lambda_I': 1e5, 'cont_weight': 100.0},\n",
    "    {'name': 'No RBA', 'rba': False, 'rad': True, 'causal': True, 'grad_norm': True, 'lambda_I': 1e5, 'cont_weight': 100.0},\n",
    "    {'name': 'No RBA, No RAD', 'rba': False, 'rad': False, 'causal': True, 'grad_norm': True, 'lambda_I': 1e5, 'cont_weight': 100.0},\n",
    "    {'name': 'No RBA, No Causal', 'rba': False, 'rad': True, 'causal': False, 'grad_norm': True, 'lambda_I': 1e5, 'cont_weight': 100.0},\n",
    "    {'name': 'No RBA, No Grad Norm', 'rba': False, 'rad': True, 'causal': True, 'grad_norm': False, 'lambda_I': 1e5, 'cont_weight': 100.0},\n",
    "]\n",
    "\n",
    "num_blocks = 6\n",
    "n_hidden = 16\n",
    "alpha = 1.0\n",
    "beta = 0.0\n",
    "\n",
    "print(f\"Training RGAKAN (alpha={alpha}, beta={beta}) for ablation study.\")\n",
    "\n",
    "for case in cases:\n",
    "    case_name = case['name']\n",
    "    print(f\"\\n--- Starting Case: {case_name} ---\")\n",
    "    \n",
    "    RESULTS[case_name] = dict()\n",
    "\n",
    "    for idx, run in enumerate([0, 7, 42]):\n",
    "        RESULTS[case_name][idx] = dict()\n",
    "        \n",
    "        # Initialize RBA weights - full pool\n",
    "        l_E_pool = jnp.ones((collocs_pool.shape[0], 1))\n",
    "        # Also get RBAs for ICs (separate for u, v, w)\n",
    "        l_I_u = jnp.ones((ic_collocs.shape[0], 1))\n",
    "        l_I_v = jnp.ones((ic_collocs.shape[0], 1))\n",
    "        l_I_w = jnp.ones((ic_collocs.shape[0], 1))\n",
    "    \n",
    "        # Get starting collocation points & RBA weights\n",
    "        sorted_indices = _get_colloc_indices(collocs_pool=collocs_pool, batch_size=batch_size, px=None, seed=seed)\n",
    "        \n",
    "        collocs = collocs_pool[sorted_indices]\n",
    "        l_E = l_E_pool[sorted_indices]\n",
    "        \n",
    "        # Get opt_type\n",
    "        opt_type = _get_adam(learning_rate=learning_rate, decay_steps=decay_steps, decay_rate=decay_rate, warmup_steps=warmup_steps)\n",
    "\n",
    "        # Define model\n",
    "        model = RGAKAN(n_in=n_in, n_out=n_out, n_hidden=n_hidden, num_blocks=num_blocks, D=D,\n",
    "                       init_scheme=init_scheme, alpha=alpha, beta=beta, ref=None,\n",
    "                       period_axes=period_axes, rff_std=None, sine_D=sine_D, seed=seed+run)\n",
    "\n",
    "        if idx == 0:\n",
    "            print(f\"Initialized model with {count_params(model)} parameters.\")\n",
    "        \n",
    "        # Define global loss weights\n",
    "        λ_E = jnp.array(1.0, dtype=float)\n",
    "        λ_I = jnp.array(case['lambda_I'], dtype=float)\n",
    "\n",
    "        # Set optimizer\n",
    "        optimizer = nnx.Optimizer(model, opt_type)\n",
    "\n",
    "        tick = time.time()\n",
    "    \n",
    "        # Start training\n",
    "        for epoch in range(num_epochs):\n",
    "        \n",
    "            loss, grads_E, grads_I, l_E, l_I_u, l_I_v, l_I_w = train_step(\n",
    "                model, optimizer, collocs, ic_collocs, u0_data, v0_data, w0_data, \n",
    "                case['rba'], case['causal'], case['cont_weight'],\n",
    "                λ_E, λ_I, l_E, l_I_u, l_I_v, l_I_w\n",
    "            )\n",
    "            \n",
    "            # Perform grad norm\n",
    "            if case['grad_norm'] and (epoch != 0) and (epoch % f_grad_norm == 0):\n",
    "                λ_Ε, λ_I = grad_norm(grads_E, grads_I, λ_E, λ_I, grad_mixing)\n",
    "        \n",
    "            # Perform RAD\n",
    "            if case['rad'] and (epoch != 0) and (epoch % f_resample == 0):\n",
    "                # Get new indices after resampling\n",
    "                sorted_indices, l_E_pool = get_RAD_indices(model, collocs_pool, sorted_indices, l_E, l_E_pool)\n",
    "                # Set new batch of collocs and l_E\n",
    "                collocs = collocs_pool[sorted_indices]\n",
    "                l_E = l_E_pool[sorted_indices]\n",
    "\n",
    "        tack = time.time()\n",
    "        \n",
    "        # Evaluate on full trajectory\n",
    "        u_err, v_err, w_err, u_pred, v_pred, w_pred = model_eval_nst(model, coords, u_sol, v_sol, w_sol)\n",
    "\n",
    "        print(f\"\\tRun = {idx}\\t L²(u)={u_err:.2e}\\t L²(v)={v_err:.2e}\\t L²(w)={w_err:.2e}\\t Loss = {loss:.2e}\\t Time = {(tack-tick)/60:.2f} mins\")\n",
    "\n",
    "        RESULTS[case_name][idx]['l2_u'] = np.asarray(device_get(u_err))\n",
    "        RESULTS[case_name][idx]['l2_v'] = np.asarray(device_get(v_err))\n",
    "        RESULTS[case_name][idx]['l2_w'] = np.asarray(device_get(w_err))\n",
    "        RESULTS[case_name][idx]['loss'] = np.asarray(device_get(loss))\n",
    "        RESULTS[case_name][idx]['time'] = (tack-tick)/num_epochs\n",
    "\n",
    "        RESULTS[case_name][idx]['u_pred'] = np.asarray(device_get(u_pred))\n",
    "        RESULTS[case_name][idx]['w_pred'] = np.asarray(device_get(w_pred))\n",
    "        RESULTS[case_name][idx]['v_pred'] = np.asarray(device_get(v_pred))\n",
    "\n",
    "    # Save intermediate results\n",
    "    with open(result_file, \"wb\") as f:\n",
    "        pickle.dump(RESULTS, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb931d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(result_file, \"wb\") as f:\n",
    "    pickle.dump(RESULTS, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bbb27d-8225-47fe-970a-9a23057c2eaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7be49a4e-40cc-457a-8790-bbc4b076aa28",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38c30de-106a-4c04-9251-d478215f4879",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(result_file, \"rb\") as f:\n",
    "    RESULTS = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91750c9-1043-48cf-b91b-233788112597",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_ablation_summary(results):\n",
    "    \"\"\"\n",
    "    Parses the RESULTS dictionary and prints a summary table with \n",
    "    Mean +/- Standard Error for L2 errors and average time per iteration.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define header format\n",
    "    header = f\"{'Ablation Type':<25} | {'L²(u)':<20} | {'L²(v)':<20} | {'L²(w)':<20} | {'Time/Iter (ms)'}\"\n",
    "    print(\"-\" * len(header))\n",
    "    print(header)\n",
    "    print(\"-\" * len(header))\n",
    "\n",
    "    for case_name, runs in results.items():\n",
    "        # 1. Aggregate data across the 3 runs for this case\n",
    "        # We assume the structure is results[case][run_idx][metric]\n",
    "        u_vals = [runs[i]['l2_u'] for i in runs]\n",
    "        v_vals = [runs[i]['l2_v'] for i in runs]\n",
    "        w_vals = [runs[i]['l2_w'] for i in runs]\n",
    "        t_vals = [runs[i]['time'] for i in runs]\n",
    "\n",
    "        # 2. Helper to get \"Mean +/- SE\" string\n",
    "        def get_stat_str(values):\n",
    "            arr = np.array(values, dtype=np.float64)\n",
    "            mean = np.mean(arr)\n",
    "            # Standard Error = Std Dev / Sqrt(N)\n",
    "            # ddof=1 for sample standard deviation\n",
    "            se = np.std(arr, ddof=1) / np.sqrt(len(arr)) \n",
    "            return f\"{mean:.2e} ± {se:.1e}\"\n",
    "\n",
    "        # 3. Calculate statistics\n",
    "        u_str = get_stat_str(u_vals)\n",
    "        v_str = get_stat_str(v_vals)\n",
    "        w_str = get_stat_str(w_vals)\n",
    "        \n",
    "        # Time is just the simple mean across runs\n",
    "        time_mean = np.mean(t_vals)*1000\n",
    "\n",
    "        # 4. Print row\n",
    "        print(f\"{case_name:<25} | {u_str:<20} | {v_str:<20} | {w_str:<20} | {time_mean:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29590ade-cf72-4a36-b321-577e059388a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Usage ---\n",
    "# Assuming 'RESULTS' is populated from your training loop:\n",
    "print_ablation_summary(RESULTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d55fc2-9823-4752-a46b-540488f1a662",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
