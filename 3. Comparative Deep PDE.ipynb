{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c2e8de4-b906-4fee-9d0c-3229574c71b8",
   "metadata": {},
   "source": [
    "# Experiment 3: Deep PDE Solving with cPIKAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2695b48-16d7-484d-9565-ccd8851af250",
   "metadata": {},
   "source": [
    "## Allen-Cahn Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5e1781-9514-460c-bd4d-954756b60985",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "jax.config.update(\"jax_default_matmul_precision\", \"highest\")\n",
    "\n",
    "from src.equations import ac_res as pde_res\n",
    "\n",
    "from src.utils import _get_adam, _get_pde_collocs, _get_ic_collocs, model_eval, count_params, _get_colloc_indices, grad_norm\n",
    "from src.kan import KAN\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import optax\n",
    "from flax import nnx\n",
    "\n",
    "import os\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "results_dir = \"results\"\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "plots_dir = \"plots\"\n",
    "os.makedirs(plots_dir, exist_ok=True)\n",
    "\n",
    "# Define the experiment\n",
    "experiment_name = \"comparative_pde\"\n",
    "results_file = os.path.join(results_dir, f\"{experiment_name}.csv\")\n",
    "\n",
    "# Define the file header\n",
    "header = \"pde, init, width, depth, run, l2\"\n",
    "\n",
    "# Check if the file exists and write the header if it doesn't\n",
    "if not os.path.exists(results_file):\n",
    "    with open(results_file, \"w\") as file:\n",
    "        file.write(header + \"\\n\")\n",
    "        \n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655960a1-288f-43be-9e39-d95c76fcbc3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8fdc3451-f9ca-485a-86a2-f9284319a7a5",
   "metadata": {},
   "source": [
    "### Basic Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6044589a-5805-46e4-97d2-f1c5878771d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDE Loss\n",
    "def pde_loss(model, l_E, collocs):\n",
    "\n",
    "    residuals = pde_res(model, collocs) # shape (batch_size, 1)\n",
    "\n",
    "    # Get new RBA weights\n",
    "    abs_res = jnp.abs(residuals)\n",
    "    l_E_new = (RBA_gamma*l_E) + (RBA_eta*abs_res/jnp.max(abs_res)) # shape (batch_size, 1)\n",
    "\n",
    "    # Multiply by RBA weights\n",
    "    w_resids = l_E_new * residuals # shape (batch_size, 1)\n",
    "\n",
    "    # Reshape residuals for causal training\n",
    "    residuals = w_resids.reshape(num_chunks, -1) # shape (num_chunks, points)\n",
    "\n",
    "    # Get average loss per chunk\n",
    "    loss = jnp.mean(residuals**2, axis=1)\n",
    "\n",
    "    # Get causal weights\n",
    "    weights = jax.lax.stop_gradient(jnp.exp(-causal_tol * (M @ loss)))\n",
    "\n",
    "    # Weighted loss\n",
    "    weighted_loss = jnp.mean(weights * loss)\n",
    "\n",
    "    return weighted_loss, l_E_new\n",
    "\n",
    "\n",
    "# IC Loss\n",
    "def ic_loss(model, l_I, ic_collocs, ic_data):\n",
    "\n",
    "    # Residual\n",
    "    ic_res = model(ic_collocs) - ic_data\n",
    "\n",
    "    # Get new RBA weights\n",
    "    abs_res = jnp.abs(ic_res)\n",
    "    l_I_new = (RBA_gamma*l_I) + (RBA_eta*abs_res/jnp.max(abs_res))\n",
    "\n",
    "    # Multiply by RBA weights\n",
    "    w_resids = l_I_new * ic_res\n",
    "\n",
    "    # Loss\n",
    "    loss = jnp.mean(w_resids**2)\n",
    "\n",
    "    return loss, l_I_new\n",
    "\n",
    "\n",
    "@nnx.jit\n",
    "def train_step(model, optimizer, collocs, ic_collocs, ic_data, λ_E, λ_I, l_E, l_I):\n",
    "\n",
    "    # PDE loss\n",
    "    (loss_E, l_E_new), grads_E = nnx.value_and_grad(pde_loss, has_aux=True)(model, l_E, collocs)\n",
    "\n",
    "    # IC loss\n",
    "    (loss_I, l_I_new), grads_I = nnx.value_and_grad(ic_loss, has_aux=True)(model, l_I, ic_collocs, ic_data)\n",
    "    \n",
    "    # Compute total loss\n",
    "    loss = λ_E*loss_E + λ_I*loss_I\n",
    "\n",
    "    # Compute total gradients\n",
    "    grads = jax.tree_util.tree_map(lambda g1, g2: λ_E * g1 + λ_I * g2, grads_E, grads_I)\n",
    "\n",
    "    # Optimizer step\n",
    "    optimizer.update(grads)\n",
    "\n",
    "    return loss, grads_E, grads_I, l_E_new, l_I_new\n",
    "\n",
    "\n",
    "@nnx.jit\n",
    "def get_RAD_indices(model, collocs_pool, old_indices, l_E, l_E_pool):\n",
    "\n",
    "    # Apply updates from old indices to pool\n",
    "    updated_pool = l_E_pool.at[old_indices].set(l_E)\n",
    "\n",
    "    # Get full residuals\n",
    "    resids = pde_res(model, collocs_pool)\n",
    "    \n",
    "    # Multiply by RBA weights\n",
    "    w_resids = updated_pool * resids\n",
    "    \n",
    "    # Get absolute\n",
    "    wa_resids = jnp.abs(w_resids)\n",
    "\n",
    "    # Raise to power rad_a\n",
    "    ea = jnp.power(wa_resids, rad_a)\n",
    "    \n",
    "    # Divide by mean and add rad_c\n",
    "    px = (ea/jnp.mean(ea)) + rad_c\n",
    "    \n",
    "    # Normalize\n",
    "    px_norm = (px / jnp.sum(px))[:,0]\n",
    "\n",
    "    sorted_indices = _get_colloc_indices(collocs_pool=collocs_pool, batch_size=batch_size, px=px_norm, seed=seed)\n",
    "\n",
    "    return sorted_indices, updated_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857d50ee-a344-4d62-aef2-65afec7cd0b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b4c59f55-5b9b-4a4a-a968-1c00d06fcf5f",
   "metadata": {},
   "source": [
    "### Data & Grid-Search Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9771ef-c0c8-4bbd-ad18-f0724bc3d7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collocation points and ICs\n",
    "collocs_pool = _get_pde_collocs(ranges = [(0,1), (-1,1)], sample_size = 400)\n",
    "ic_collocs = _get_ic_collocs(x_range = (-1, 1), sample_size = 2**6)\n",
    "\n",
    "# Custom for each PDE\n",
    "ic_data = ((ic_collocs[:,1]**2)*jnp.cos(jnp.pi*ic_collocs[:,1])).reshape(-1,1)\n",
    "\n",
    "# Reference solution\n",
    "ref = np.load('data/allen_cahn.npz')\n",
    "refsol = jnp.array(ref['usol'])\n",
    "\n",
    "N_t, N_x = ref['usol'].shape\n",
    "t, x = ref['t'].flatten(), ref['x'].flatten()\n",
    "T, X = jnp.meshgrid(t, x, indexing='ij')\n",
    "coords = jnp.hstack([T.flatten()[:, None], X.flatten()[:, None]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42817b15-6362-4854-88c7-4b863c6b04dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training epochs\n",
    "num_epochs = 100_000\n",
    "\n",
    "# Scheduler configurations\n",
    "learning_rate = 1e-3\n",
    "decay_steps = 2000\n",
    "decay_rate = 0.9\n",
    "warmup_steps = 1000\n",
    "\n",
    "# Define causal training parameters\n",
    "causal_tol = 1.0\n",
    "num_chunks = 32\n",
    "M = jnp.triu(jnp.ones((num_chunks, num_chunks)), k=1).T\n",
    "\n",
    "# Define Grad Norm parameters\n",
    "grad_mixing = 0.9\n",
    "f_grad_norm = 1000\n",
    "\n",
    "# Define resampling parameters\n",
    "batch_size = 2**12\n",
    "f_resample = 2000\n",
    "rad_a = 1.0\n",
    "rad_c = 1.0\n",
    "\n",
    "# Define RBA parameters\n",
    "RBA_gamma = 0.999\n",
    "RBA_eta = 0.01\n",
    "\n",
    "# Define model parameters\n",
    "n_in = collocs_pool.shape[1]\n",
    "n_out = 1\n",
    "D = 5\n",
    "period_axes = {1: jnp.pi}\n",
    "rff_std = None\n",
    "\n",
    "glorot_init = {'type': 'glorot', 'gain': None, 'norm_pow': 0, 'distribution': 'uniform', 'sample_size': 10000}\n",
    "default_init = {'type': 'default'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a212d91e-24c7-4d04-868c-1953092eb66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Architecture settings\n",
    "widths = [8, 16, 32]\n",
    "depths = [2, 4, 6, 8, 10, 12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58b269a-f2e2-40fc-a6cc-f473df372872",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a75d7b70-6bc6-4501-8e9d-4f53ea2d7321",
   "metadata": {},
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76428312-c319-4121-a322-56bed8a15094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procedure\n",
    "for init_scheme in [default_init, glorot_init]:\n",
    "\n",
    "    scheme_title = \"glorot\" if init_scheme == glorot_init else \"default\"\n",
    "\n",
    "    # Grid search over depth size\n",
    "    for num_layers in depths:\n",
    "\n",
    "        # Grid search over width size\n",
    "        for n_hidden in widths:\n",
    "            \n",
    "            print(f\"Training model with depth = {num_layers} and width = {n_hidden} (Initilization = {scheme_title}).\")\n",
    "\n",
    "            for run in [3, 5, 6, 7, 13]:\n",
    "\n",
    "                # Initialize RBA weights - full pool\n",
    "                l_E_pool = jnp.ones((collocs_pool.shape[0], 1))\n",
    "                # Also get RBAs for ICs\n",
    "                l_I = jnp.ones((ic_collocs.shape[0], 1))\n",
    "            \n",
    "                # Get starting collocation points & RBA weights\n",
    "                sorted_indices = _get_colloc_indices(collocs_pool=collocs_pool, batch_size=batch_size, px=None, seed=seed)\n",
    "                \n",
    "                collocs = collocs_pool[sorted_indices]\n",
    "                l_E = l_E_pool[sorted_indices]\n",
    "                \n",
    "                # Get opt_type\n",
    "                opt_type = _get_adam(learning_rate=learning_rate, decay_steps=decay_steps, decay_rate=decay_rate, warmup_steps=warmup_steps)\n",
    "\n",
    "                # Define model\n",
    "                model = KAN(n_in = n_in, n_out = n_out, n_hidden = n_hidden, num_layers = num_layers, D = D,\n",
    "                            init_scheme = init_scheme, period_axes = period_axes, rff_std = rff_std,\n",
    "                            seed = seed+run)\n",
    "\n",
    "                if run == 1:\n",
    "                    print(f\"Initialized model with {count_params(model)} parameters.\")\n",
    "                \n",
    "                # Define global loss weights\n",
    "                λ_E = jnp.array(1.0, dtype=float)\n",
    "                λ_I = jnp.array(1.0, dtype=float)\n",
    "\n",
    "                # Set optimizer\n",
    "                optimizer = nnx.Optimizer(model, opt_type)\n",
    "            \n",
    "                # Start training\n",
    "                for epoch in range(num_epochs):\n",
    "                \n",
    "                    loss, grads_E, grads_I, l_E, l_I = train_step(model, optimizer, collocs, ic_collocs, ic_data, λ_E, λ_I, l_E, l_I)\n",
    "                \n",
    "                    # Perform grad norm\n",
    "                    if (epoch != 0) and (epoch % f_grad_norm == 0):\n",
    "                \n",
    "                        λ_Ε, λ_I = grad_norm(grads_E, grads_I, λ_E, λ_I, grad_mixing)\n",
    "                \n",
    "                    # Perform RAD\n",
    "                    if (epoch != 0) and (epoch % f_resample == 0):\n",
    "            \n",
    "                        # Get new indices after resampling\n",
    "                        sorted_indices, l_E_pool = get_RAD_indices(model, collocs_pool, sorted_indices, l_E, l_E_pool)\n",
    "                        # Set new batch of collocs and l_E\n",
    "                        collocs = collocs_pool[sorted_indices]\n",
    "                        l_E = l_E_pool[sorted_indices]\n",
    "            \n",
    "                # Evaluate\n",
    "                output = model(coords).reshape(refsol.shape)\n",
    "                l2error = jnp.linalg.norm(output-refsol)/jnp.linalg.norm(refsol)\n",
    "            \n",
    "                # Log results\n",
    "                new_row = f\"ac, {scheme_title}, {n_hidden}, {num_layers}, {run}, {l2error}\"\n",
    "                                \n",
    "                # Append the row to the file\n",
    "                with open(results_file, \"a\") as rfile:\n",
    "                    rfile.write(new_row + \"\\n\")\n",
    "\n",
    "                print(f\"\\t{run}. Final Rel. L2 Error: {l2error:.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd5d2d2-5a97-42c6-a59b-f044b426e410",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f16b956-2e9f-40af-948e-49c208f9e464",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c7e167f-308e-4d94-b1f9-9bdd007c2508",
   "metadata": {},
   "source": [
    "## Burgers' Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56864f21-f835-48f6-963c-a403d8b00193",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.equations import burgers_res as pde_res\n",
    "\n",
    "from src.wrappers import BurgersKAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e484dc42-a161-451b-9a51-4d1947f64813",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6628b818-3f31-4cc9-9c27-278d6872e62b",
   "metadata": {},
   "source": [
    "### Basic Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580bdc23-3a14-4377-8def-538001d323ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDE Loss\n",
    "def pde_loss(model, l_E, collocs):\n",
    "\n",
    "    residuals = pde_res(model, collocs) # shape (batch_size, 1)\n",
    "\n",
    "    # Get new RBA weights\n",
    "    abs_res = jnp.abs(residuals)\n",
    "    l_E_new = (RBA_gamma*l_E) + (RBA_eta*abs_res/jnp.max(abs_res)) # shape (batch_size, 1)\n",
    "\n",
    "    # Multiply by RBA weights\n",
    "    w_resids = l_E_new * residuals # shape (batch_size, 1)\n",
    "\n",
    "    # Reshape residuals for causal training\n",
    "    residuals = w_resids.reshape(num_chunks, -1) # shape (num_chunks, points)\n",
    "\n",
    "    # Get average loss per chunk\n",
    "    loss = jnp.mean(residuals**2, axis=1)\n",
    "\n",
    "    # Get causal weights\n",
    "    weights = jax.lax.stop_gradient(jnp.exp(-causal_tol * (M @ loss)))\n",
    "\n",
    "    # Weighted loss\n",
    "    weighted_loss = jnp.mean(weights * loss)\n",
    "\n",
    "    return weighted_loss, l_E_new\n",
    "\n",
    "\n",
    "# IC Loss\n",
    "def ic_loss(model, l_I, ic_collocs, ic_data):\n",
    "\n",
    "    # Residual\n",
    "    ic_res = model(ic_collocs) - ic_data\n",
    "\n",
    "    # Get new RBA weights\n",
    "    abs_res = jnp.abs(ic_res)\n",
    "    l_I_new = (RBA_gamma*l_I) + (RBA_eta*abs_res/jnp.max(abs_res))\n",
    "\n",
    "    # Multiply by RBA weights\n",
    "    w_resids = l_I_new * ic_res\n",
    "\n",
    "    # Loss\n",
    "    loss = jnp.mean(w_resids**2)\n",
    "\n",
    "    return loss, l_I_new\n",
    "\n",
    "\n",
    "@nnx.jit\n",
    "def train_step(model, optimizer, collocs, ic_collocs, ic_data, λ_E, λ_I, l_E, l_I):\n",
    "\n",
    "    # PDE loss\n",
    "    (loss_E, l_E_new), grads_E = nnx.value_and_grad(pde_loss, has_aux=True)(model, l_E, collocs)\n",
    "\n",
    "    # IC loss\n",
    "    (loss_I, l_I_new), grads_I = nnx.value_and_grad(ic_loss, has_aux=True)(model, l_I, ic_collocs, ic_data)\n",
    "    \n",
    "    # Compute total loss\n",
    "    loss = λ_E*loss_E + λ_I*loss_I\n",
    "\n",
    "    # Compute total gradients\n",
    "    grads = jax.tree_util.tree_map(lambda g1, g2: λ_E * g1 + λ_I * g2, grads_E, grads_I)\n",
    "\n",
    "    # Optimizer step\n",
    "    optimizer.update(grads)\n",
    "\n",
    "    return loss, grads_E, grads_I, l_E_new, l_I_new\n",
    "\n",
    "\n",
    "@nnx.jit\n",
    "def get_RAD_indices(model, collocs_pool, old_indices, l_E, l_E_pool):\n",
    "\n",
    "    # Apply updates from old indices to pool\n",
    "    updated_pool = l_E_pool.at[old_indices].set(l_E)\n",
    "\n",
    "    # Get full residuals\n",
    "    resids = pde_res(model, collocs_pool)\n",
    "    \n",
    "    # Multiply by RBA weights\n",
    "    w_resids = updated_pool * resids\n",
    "    \n",
    "    # Get absolute\n",
    "    wa_resids = jnp.abs(w_resids)\n",
    "\n",
    "    # Raise to power rad_a\n",
    "    ea = jnp.power(wa_resids, rad_a)\n",
    "    \n",
    "    # Divide by mean and add rad_c\n",
    "    px = (ea/jnp.mean(ea)) + rad_c\n",
    "    \n",
    "    # Normalize\n",
    "    px_norm = (px / jnp.sum(px))[:,0]\n",
    "\n",
    "    sorted_indices = _get_colloc_indices(collocs_pool=collocs_pool, batch_size=batch_size, px=px_norm, seed=seed)\n",
    "\n",
    "    return sorted_indices, updated_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77e2424-76de-462b-88ec-eb1ebebd2832",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aff6efeb-6423-4d04-b155-618c0b53e20b",
   "metadata": {},
   "source": [
    "### Data & Grid-Search Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5de24f-1a11-4ac8-b1ae-770ba7461061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collocation points and ICs\n",
    "collocs_pool = _get_pde_collocs(ranges = [(0,1), (-1,1)], sample_size = 400)\n",
    "ic_collocs = _get_ic_collocs(x_range = (-1, 1), sample_size = 2**6)\n",
    "\n",
    "# Custom for each PDE\n",
    "ic_data = -jnp.sin(jnp.pi*ic_collocs[:,1]).reshape(-1,1)\n",
    "\n",
    "# Reference solution\n",
    "ref = np.load('data/burgers.npz')\n",
    "refsol = jnp.array(ref['usol'])\n",
    "\n",
    "N_t, N_x = ref['usol'].shape\n",
    "t, x = ref['t'].flatten(), ref['x'].flatten()\n",
    "T, X = jnp.meshgrid(t, x, indexing='ij')\n",
    "coords = jnp.hstack([T.flatten()[:, None], X.flatten()[:, None]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99287b88-20a6-415a-8193-268d8fa73c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training epochs\n",
    "num_epochs = 100_000\n",
    "\n",
    "# Scheduler configurations\n",
    "learning_rate = 1e-3\n",
    "decay_steps = 2000\n",
    "decay_rate = 0.9\n",
    "warmup_steps = 1000\n",
    "\n",
    "# Define causal training parameters\n",
    "causal_tol = 1.0\n",
    "num_chunks = 32\n",
    "M = jnp.triu(jnp.ones((num_chunks, num_chunks)), k=1).T\n",
    "\n",
    "# Define Grad Norm parameters\n",
    "grad_mixing = 0.9\n",
    "f_grad_norm = 1000\n",
    "\n",
    "# Define resampling parameters\n",
    "batch_size = 2**12\n",
    "f_resample = 2000\n",
    "rad_a = 1.0\n",
    "rad_c = 1.0\n",
    "\n",
    "# Define RBA parameters\n",
    "RBA_gamma = 0.999\n",
    "RBA_eta = 0.01\n",
    "\n",
    "# Define model parameters\n",
    "n_in = collocs_pool.shape[1]\n",
    "n_out = 1\n",
    "D = 5\n",
    "period_axes = None\n",
    "rff_std = None\n",
    "\n",
    "glorot_init = {'type': 'glorot', 'gain': None, 'norm_pow': 0, 'distribution': 'uniform', 'sample_size': 10000}\n",
    "default_init = {'type': 'default'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2f0695-1b3b-47b5-a12e-f4284ad4441d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Architecture settings\n",
    "widths = [8, 16, 32]\n",
    "depths = [2, 4, 6, 8, 10, 12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216de684-db45-4394-a07c-2c429afc8091",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cfc03080-76c5-470a-b1b9-d59c5ce03d1e",
   "metadata": {},
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60640ab4-9065-4102-b70a-37ab994d79ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procedure\n",
    "for init_scheme in [default_init, glorot_init]:\n",
    "\n",
    "    scheme_title = \"glorot\" if init_scheme == glorot_init else \"default\"\n",
    "\n",
    "    # Grid search over depth size\n",
    "    for num_layers in depths:\n",
    "\n",
    "        # Grid search over width size\n",
    "        for n_hidden in widths:\n",
    "            \n",
    "            print(f\"Training model with depth = {num_layers} and width = {n_hidden} (Initilization = {scheme_title}).\")\n",
    "\n",
    "            for run in [3, 5, 6, 7, 13]:\n",
    "\n",
    "                # Initialize RBA weights - full pool\n",
    "                l_E_pool = jnp.ones((collocs_pool.shape[0], 1))\n",
    "                # Also get RBAs for ICs\n",
    "                l_I = jnp.ones((ic_collocs.shape[0], 1))\n",
    "            \n",
    "                # Get starting collocation points & RBA weights\n",
    "                sorted_indices = _get_colloc_indices(collocs_pool=collocs_pool, batch_size=batch_size, px=None, seed=seed)\n",
    "                \n",
    "                collocs = collocs_pool[sorted_indices]\n",
    "                l_E = l_E_pool[sorted_indices]\n",
    "                \n",
    "                # Get opt_type\n",
    "                opt_type = _get_adam(learning_rate=learning_rate, decay_steps=decay_steps, decay_rate=decay_rate, warmup_steps=warmup_steps)\n",
    "\n",
    "                # Define model\n",
    "                model = BurgersKAN(n_in = n_in, n_out = n_out, n_hidden = n_hidden, num_layers = num_layers, D = D,\n",
    "                            init_scheme = init_scheme, period_axes = period_axes, rff_std = rff_std,\n",
    "                            seed = seed+run)\n",
    "\n",
    "                if run == 1:\n",
    "                    print(f\"Initialized model with {count_params(model)} parameters.\")\n",
    "                \n",
    "                # Define global loss weights\n",
    "                λ_E = jnp.array(1.0, dtype=float)\n",
    "                λ_I = jnp.array(1.0, dtype=float)\n",
    "\n",
    "                # Set optimizer\n",
    "                optimizer = nnx.Optimizer(model, opt_type)\n",
    "            \n",
    "                # Start training\n",
    "                for epoch in range(num_epochs):\n",
    "                \n",
    "                    loss, grads_E, grads_I, l_E, l_I = train_step(model, optimizer, collocs, ic_collocs, ic_data, λ_E, λ_I, l_E, l_I)\n",
    "                \n",
    "                    # Perform grad norm\n",
    "                    if (epoch != 0) and (epoch % f_grad_norm == 0):\n",
    "                \n",
    "                        λ_Ε, λ_I = grad_norm(grads_E, grads_I, λ_E, λ_I, grad_mixing)\n",
    "                \n",
    "                    # Perform RAD\n",
    "                    if (epoch != 0) and (epoch % f_resample == 0):\n",
    "            \n",
    "                        # Get new indices after resampling\n",
    "                        sorted_indices, l_E_pool = get_RAD_indices(model, collocs_pool, sorted_indices, l_E, l_E_pool)\n",
    "                        # Set new batch of collocs and l_E\n",
    "                        collocs = collocs_pool[sorted_indices]\n",
    "                        l_E = l_E_pool[sorted_indices]\n",
    "            \n",
    "                # Evaluate\n",
    "                output = model(coords).reshape(refsol.shape)\n",
    "                l2error = jnp.linalg.norm(output-refsol)/jnp.linalg.norm(refsol)\n",
    "            \n",
    "                # Log results\n",
    "                new_row = f\"burgers, {scheme_title}, {n_hidden}, {num_layers}, {run}, {l2error}\"\n",
    "                                \n",
    "                # Append the row to the file\n",
    "                with open(results_file, \"a\") as rfile:\n",
    "                    rfile.write(new_row + \"\\n\")\n",
    "\n",
    "                print(f\"\\t{run}. Final Rel. L2 Error: {l2error:.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527e2929-bdd0-4da9-8458-4fd45d0e0029",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fba5fb84-9c87-49a3-a035-e1288df0a873",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17a0910-a7d8-4bec-a18c-bbbfdc473e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.ParserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea371213-1847-4068-847d-e215a43804ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import LogLocator, LogFormatterMathtext, NullLocator, NullFormatter\n",
    "import seaborn as sns\n",
    "\n",
    "# ===== configurable knobs =====\n",
    "TITLE_FS   = 16\n",
    "LABEL_FS   = 16\n",
    "TICK_FS    = 14\n",
    "LEGEND_FS  = 16\n",
    "LINE_W     = 2.0\n",
    "ALPHA_FILL = 0.25\n",
    "PDE_ORDER  = [\"burgers\", \"ac\"]      # row order (change if you like)\n",
    "WIDTHS     = [8, 16, 32]            # column order\n",
    "INIT_LABEL = {\"glorot\": \"Proposed Initialization\", \"default\": \"Default Initialization\"}\n",
    "PDE_LABEL = {\"ac\": \"Allen-Cahn\", \"burgers\": \"Burgers\"}\n",
    "\n",
    "# color choice (2 distinct tones from a seaborn colormap)\n",
    "_cmap = sns.color_palette(\"Spectral\", as_cmap=True)\n",
    "cmap_points = np.linspace(0, 1, 12)\n",
    "\n",
    "color_indices = [1, -2]\n",
    "_colors = [_cmap(cmap_points[i]) for i in color_indices]\n",
    "\n",
    "def _set_log_ticks(ax):\n",
    "    ax.set_yscale(\"log\")\n",
    "    ax.yaxis.set_major_locator(LogLocator(base=10.0))\n",
    "    ax.yaxis.set_major_formatter(LogFormatterMathtext(base=10.0))\n",
    "    ax.yaxis.set_minor_locator(NullLocator())\n",
    "    ax.yaxis.set_minor_formatter(NullFormatter())\n",
    "    ax.tick_params(axis=\"both\", which=\"both\", labelsize=TICK_FS)\n",
    "\n",
    "def _plot_mean_sem(ax, x, y_mean, y_sem, label, color):\n",
    "    line, = ax.plot(x, y_mean, label=label, linewidth=LINE_W, color=color, marker=\"o\")\n",
    "    ax.fill_between(x, y_mean - y_sem, y_mean + y_sem, alpha=ALPHA_FILL, color=color, linewidth=0)\n",
    "    return line\n",
    "\n",
    "def _prep_stats(df):\n",
    "    # expect columns: pde, init, width, depth, run, l2\n",
    "    g = df.groupby([\"pde\", \"init\", \"width\", \"depth\"])[\"l2\"].agg([\"mean\", \"sem\"]).reset_index()\n",
    "    # ensure depth is sorted numerically\n",
    "    g = g.sort_values([\"pde\", \"width\", \"init\", \"depth\"]).reset_index(drop=True)\n",
    "    return g\n",
    "\n",
    "def plot_pde_grid(df):\n",
    "\n",
    "    stats = _prep_stats(df)\n",
    "\n",
    "    # ensure presence/ordering\n",
    "    pdes   = [p for p in PDE_ORDER if p in stats[\"pde\"].unique()]\n",
    "    widths = [w for w in WIDTHS    if w in stats[\"width\"].unique()]\n",
    "    inits  = [i for i in [\"default\", \"glorot\"] if i in stats[\"init\"].unique()]\n",
    "\n",
    "    nrows, ncols = len(pdes), len(widths)\n",
    "    if nrows == 0 or ncols == 0 or not inits:\n",
    "        raise ValueError(\"No matching pdes/widths/inits to plot after filtering.\")\n",
    "\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows, ncols, figsize=(4 * ncols, 2.5 * nrows),\n",
    "        sharex=True, sharey=False, constrained_layout=True\n",
    "    )\n",
    "\n",
    "    fig.set_constrained_layout_pads(w_pad=0.1, h_pad=0.1, hspace=0.06, wspace=0.06)\n",
    "    \n",
    "    # normalize axes to 2D array\n",
    "    if nrows == 1 and ncols == 1:\n",
    "        axes = np.array([[axes]])\n",
    "    elif nrows == 1:\n",
    "        axes = axes[np.newaxis, :]\n",
    "    elif ncols == 1:\n",
    "        axes = axes[:, np.newaxis]\n",
    "\n",
    "    legend_handles = None\n",
    "\n",
    "    for r, pde in enumerate(pdes):\n",
    "        for c, width in enumerate(widths):\n",
    "            ax = axes[r, c]\n",
    "            _set_log_ticks(ax)\n",
    "\n",
    "            # depths available for this (pde, width); keep numeric ascending order\n",
    "            depths_here = (\n",
    "                stats[(stats.pde == pde) & (stats.width == width)][\"depth\"]\n",
    "                .drop_duplicates().sort_values().to_numpy()\n",
    "            )\n",
    "\n",
    "            # plot each init\n",
    "            init_lines = []\n",
    "            for j, init in enumerate(inits):\n",
    "                sub = stats[(stats.pde == pde) & (stats.width == width) & (stats.init == init)]\n",
    "                if sub.empty:\n",
    "                    continue\n",
    "                # align to complete x (depth) set to avoid broken fills\n",
    "                sub = sub.set_index(\"depth\").reindex(depths_here)\n",
    "                if sub[\"mean\"].isna().all():\n",
    "                    continue\n",
    "                h = _plot_mean_sem(\n",
    "                    ax, depths_here, sub[\"mean\"].to_numpy(), sub[\"sem\"].fillna(0).to_numpy(),\n",
    "                    label=INIT_LABEL.get(init, init.title()), color=_colors[j % len(_colors)]\n",
    "                )\n",
    "                init_lines.append(h)\n",
    "\n",
    "            # titles on top row, row labels on leftmost col\n",
    "            if r == 0:\n",
    "                ax.set_title(f\"Width = {width}\", fontsize=TITLE_FS)\n",
    "            if c == 0:\n",
    "                ax.set_ylabel(r\"Relative $L^2$ Error\", fontsize=LABEL_FS)\n",
    "\n",
    "            # x label only on bottom row\n",
    "            if r == nrows - 1:\n",
    "                ax.set_xlabel(\"Depth\", fontsize=LABEL_FS)\n",
    "\n",
    "            # fix depth ticks\n",
    "            ax.set_xticks([2, 4, 6, 8, 10, 12])\n",
    "            ax.set_xticklabels([2, 4, 6, 8, 10, 12])\n",
    "\n",
    "            ax.grid(True, which=\"both\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "            # left annotate PDE name per row (nice, unobtrusive)\n",
    "            if c == ncols - 1:\n",
    "                ax.annotate(\n",
    "                    PDE_LABEL.get(pde, pde),\n",
    "                    xy=(1.08, 0.5), xycoords=\"axes fraction\",\n",
    "                    va=\"center\", ha=\"left\", rotation=90, fontsize=LABEL_FS\n",
    "                )\n",
    "\n",
    "            if legend_handles is None and init_lines:\n",
    "                legend_handles = init_lines\n",
    "\n",
    "    if legend_handles:\n",
    "        fig.legend(\n",
    "            legend_handles, [h.get_label() for h in legend_handles],\n",
    "            loc=\"lower center\", ncol=len(legend_handles), frameon=False, fontsize=LEGEND_FS,\n",
    "            bbox_to_anchor=(0.5, -0.12)\n",
    "        )\n",
    "\n",
    "    plt.savefig(f\"{plots_dir}/compar_pdes.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551a1fea-3489-4e35-90f6-c994c1ee11eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"results/comparative_pde.csv\", sep=', ')\n",
    "plot_pde_grid(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47260cd-b848-4f20-b2c7-48d2b2095c74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
